

# **심층 생성 모델링의 진화: 적대적 학습과 변분 추론에서 확률 미분 방정식을 통한 점수 기반 통합까지**

## **1\. 서론: 생성 모델링의 패러다임 전환**

기계 학습의 역사에서 데이터의 분포를 학습하고 이를 바탕으로 새로운 데이터를 생성하는 문제는 항상 핵심적인 도전 과제였습니다. 초기 통계적 방법론에서 출발하여 심층 신경망(Deep Neural Networks)의 도입으로 비약적인 발전을 이룬 생성 모델(Generative Models)은 이제 단순한 데이터 복제를 넘어 데이터의 잠재적 구조(latent structure)를 이해하고 창의적인 콘텐츠를 생산하는 수준에 도달했습니다. 본 보고서는 이러한 발전의 궤적을 추적하며, 생성적 적대 신경망(Generative Adversarial Nets, GAN), 변분 오토인코더(Variational Auto-Encoders, VAE), 적대적 오토인코더(Adversarial Autoencoders, AAE)와 같은 초기 심층 생성 모델들의 이론적 토대와 한계를 분석합니다. 나아가 최근 생성 모델링의 새로운 표준으로 부상하고 있는 점수 기반 생성 모델(Score-Based Generative Models)과 디노이징 확산 확률 모델(Denoising Diffusion Probabilistic Models, DDPM)을 심도 있게 다루며, 이들이 확률 미분 방정식(Stochastic Differential Equations, SDE)이라는 수학적 프레임워크 안에서 어떻게 통합되는지를 규명합니다.

본 연구는 단순히 각 모델의 성능을 나열하는 것을 넘어, 각 모델이 제안된 배경에 깔린 데이터 분포에 대한 가정, 최적화 과정에서의 난제(예: 모드 붕괴, 흐릿한 생성), 그리고 이를 극복하기 위해 도입된 수학적 기법(예: 재매개변수화 트릭, 랑주뱅 동역학)을 상세히 분석합니다. 특히, 데이터가 고차원 공간 내의 저차원 매니폴드에 존재한다는 '매니폴드 가설(Manifold Hypothesis)'이 기존 우도 기반 모델에 미친 영향과, 이를 해결하기 위해 등장한 노이즈 섭동(noise perturbation) 전략이 어떻게 점수 기반 모델링의 핵심이 되었는지를 중점적으로 논의합니다. 또한, SDE 기반 프레임워크가 제공하는 정확한 우도 계산, 역문제(inverse problem) 해결 능력, 그리고 제어 가능한 생성(controllable generation)과 같은 확장된 기능들이 미래 인공지능 연구에 시사하는 바를 탐구합니다.

## **2\. 생성적 적대 신경망 (GAN): 적대적 게임을 통한 분포 학습**

### **2.1 미니맥스 게임 이론과 최적화**

생성적 적대 신경망(GAN)은 두 개의 신경망, 생성자(Generator, $G$)와 판별자(Discriminator, $D$) 간의 적대적 경쟁을 통해 데이터 분포를 학습하는 혁신적인 프레임워크입니다. 이 구조는 게임 이론의 미니맥스(minimax) 정리에 기반을 두고 있으며, 생성자는 판별자를 속이기 위해 실제 데이터와 유사한 샘플을 생성하려 하고, 판별자는 입력된 샘플이 실제 데이터인지 생성된 가짜 데이터인지를 구별하려 합니다.1

이 경쟁 과정은 다음과 같은 가치 함수 $V(G, D)$에 대한 미니맥스 게임으로 정식화됩니다:

$$\\min\_{G} \\max\_{D} V(D, G) \= \\mathbb{E}\_{x \\sim p\_{data}(x)} \+ \\mathbb{E}\_{z \\sim p\_{z}(z)}$$

여기서 $p\_{data}(x)$는 실제 데이터의 분포이고, $p\_{z}(z)$는 잠재 공간(latent space)에서 정의된 사전 노이즈 분포(일반적으로 가우시안 또는 균등 분포)입니다.1 생성자 $G$는 잠재 변수 $z$를 데이터 공간으로 매핑하는 미분 가능한 함수로 정의되며, 다층 퍼셉트론(Multilayer Perceptron, MLP)으로 구현됩니다. 이 구조의 가장 큰 장점은 마르코프 체인(Markov Chain)이나 복잡한 근사 추론 네트워크 없이, 오직 역전파(Backpropagation) 알고리즘만을 사용하여 모델을 훈련할 수 있다는 점입니다.1

### **2.2 이론적 수렴성과 글로벌 최적해**

GAN의 이론적 분석은 생성자와 판별자가 충분한 용량(capacity)을 가질 때, 이 게임이 글로벌 최적해에 도달할 수 있음을 보장합니다. 판별자 $D$를 고정한 상태에서 최적의 판별자 $D^\_G(x)$는 다음과 같이 유도됩니다:

$$D^G(x) \= \\frac{p{data}(x)}{p\_{data}(x) \+ p\_{g}(x)}$$

여기서 $p\_g(x)$는 생성자 $G$에 의해 유도된 모델 분포입니다.1 이 최적의 판별자를 가치 함수에 대입하면, 생성자 $G$를 최적화하는 문제는 실제 데이터 분포 $p\_{data}$와 생성 모델 분포 $p\_{g}$ 사이의 젠슨-섀넌 발산(Jensen-Shannon Divergence, JSD)을 최소화하는 것과 수학적으로 동치임이 증명되었습니다:

$$C(G) \= \-\\log 4 \+ 2 \\cdot JSD(p\_{data} \\| p\_{g})$$

따라서 젠슨-섀넌 발산의 특성에 따라 $p\_{g} \= p\_{data}$일 때 전역 최소값 $-\\log 4$를 가지게 되며, 이때 최적의 판별자는 모든 입력에 대해 $D(x) \= \\frac{1}{2}$을 출력하게 됩니다.1 이는 판별자가 더 이상 실제 데이터와 생성된 데이터를 구별할 수 없는 평형 상태에 도달했음을 의미합니다.

### **2.3 실험적 성과와 한계점**

GAN은 MNIST, Toronto Face Database (TFD), CIFAR-10 등 다양한 데이터셋에서 기존 모델들을 능가하는 날카롭고 선명한 이미지를 생성하는 능력을 보여주었습니다.1 특히 잠재 공간 $z$에서의 선형 보간 실험은 생성자가 단순히 훈련 데이터를 암기한 것이 아니라, 데이터의 의미론적 특징을 학습하여 부드러운 변환을 수행할 수 있음을 입증했습니다.

그러나 이러한 성과에도 불구하고 GAN은 몇 가지 근본적인 한계를 가지고 있습니다:

1. **학습의 불안정성:** 판별자와 생성자의 균형을 맞추는 것이 매우 어렵습니다. 판별자가 너무 빨리 최적화되면 생성자에게 유의미한 그라디언트를 제공하지 못해 학습이 중단될 수 있습니다.1  
2. **모드 붕괴(Mode Collapse):** 생성자가 데이터 분포의 다양성을 모두 포착하지 못하고, 판별자를 속이기 쉬운 특정 샘플(모드)들만 반복적으로 생성하는 현상이 발생합니다.1 이는 $p\_g$가 $p\_{data}$의 전체 서포트(support)를 커버하지 못하는 문제를 야기합니다.  
3. **우도 평가의 어려움:** $p\_g(x)$가 암시적으로 정의되기 때문에, 모델의 우도(likelihood)를 직접 계산할 수 없습니다. 대신 Parzen window density estimation과 같은 근사적인 방법에 의존해야 하는데, 이는 고차원 공간에서 부정확할 수 있습니다.1

이러한 GAN의 한계는 명시적인 우도 기반 모델이나 더 안정적인 학습 목표를 가진 모델에 대한 연구 필요성을 제기했습니다.

## **3\. 변분 오토인코더 (VAE): 확률적 추론과 생성의 결합**

### **3.1 변분 하한과 재매개변수화 트릭**

변분 오토인코더(VAE)는 생성 모델링 문제를 확률적 관점에서 접근하며, 데이터가 관측되지 않은 연속 잠재 변수 $z$로부터 생성된다고 가정합니다. VAE는 사후 분포 $p\_{\\theta}(z|x)$가 다루기 힘든(intractable) 경우에도 효율적인 추론과 학습을 가능하게 하는 프레임워크를 제공합니다.1 이를 위해 진정한 사후 분포를 근사하는 인식 모델(recognition model) 또는 인코더 $q\_{\\phi}(z|x)$를 도입합니다.

VAE의 가장 중요한 기여는 변분 하한(Evidence Lower Bound, ELBO)을 최적화하기 위해 도입된 \*\*재매개변수화 트릭(Reparameterization Trick)\*\*입니다. 기존의 변분 추론 방식은 기댓값 내부의 분포 파라미터에 대한 미분을 계산하기 어려워 분산이 높은 몬테카를로 추정량을 사용해야 했습니다. VAE는 잠재 변수 $z$를 결정론적 함수와 보조 노이즈 변수 $\\epsilon$으로 다시 표현함으로써 이 문제를 해결했습니다 1:

$$\\tilde{z} \= g\_{\\phi}(\\epsilon, x) \\quad \\text{where} \\quad \\epsilon \\sim p(\\epsilon)$$

예를 들어, 가우시안 분포의 경우 $z \= \\mu \+ \\sigma \\odot \\epsilon$ (여기서 $\\epsilon \\sim \\mathcal{N}(0, I)$)으로 표현하여, 미분이 $\\phi$에 대해 직접 흐를 수 있게 만들었습니다. 이는 역전파를 통한 효율적인 엔드-투-엔드 학습을 가능하게 했습니다.

### **3.2 AEVB 알고리즘과 모델 구조**

VAE의 학습 알고리즘인 AEVB(Auto-Encoding Variational Bayes)는 SGVB(Stochastic Gradient Variational Bayes) 추정량을 사용하여 모델 파라미터를 업데이트합니다. 목적 함수는 다음과 같이 주어집니다:  

$$ \\mathcal{L}(\\theta, \\phi; x^{(i)}) \= \-D\_{KL}(q\_{\\phi}(z|x^{(i)}) |  
| p\_{\\theta}(z)) \+ \\mathbb{E}{q{\\phi}(z|x^{(i)})}\[\\log p\_{\\theta}(x^{(i)}|z)\] $$

첫 번째 항은 잠재 공간의 분포를 사전 분포(주로 표준 정규분포)와 가깝게 유지하려는 정규화(regularization) 역할을 하며, 두 번째 항은 입력 데이터를 잘 복원하려는 재구성 오차(reconstruction error)에 해당합니다.1 이 구조는 인코더가 데이터를 압축된 잠재 코드로 변환하고, 디코더가 이를 다시 데이터로 복원하는 오토인코더 구조와 유사하지만, 잠재 공간에 확률적 속성을 부여했다는 점에서 차별화됩니다.

### **3.3 실험 결과 및 분석**

VAE는 MNIST와 Frey Face 데이터셋에서 기존의 Wake-Sleep 알고리즘보다 더 빠른 수렴 속도와 더 높은 변분 하한 값을 달성했습니다.1 실험 결과, 잠재 공간의 차원을 늘려도 과적합(overfitting)이 발생하지 않는 특성을 보였는데, 이는 변분 하한의 정규화 효과 때문으로 분석됩니다. 또한, 2차원 잠재 공간 시각화를 통해 VAE가 데이터의 매니폴드를 부드럽게 학습하고 있음을 확인할 수 있었습니다. 그러나 VAE는 픽셀 단위의 평균 제곱 오차나 교차 엔트로피를 사용하여 재구성 오차를 최소화하기 때문에, 생성된 이미지가 다소 흐릿(blurry)하게 나타나는 경향이 있습니다. 이는 고주파 세부 정보(texture, edge 등)를 잃어버리는 원인이 되며, GAN과 대비되는 단점 중 하나입니다.

## **4\. 적대적 오토인코더 (AAE): 변분 추론의 적대적 확장**

### **4.1 잠재 공간의 정규화와 적대적 학습**

적대적 오토인코더(AAE)는 VAE의 확률적 인코딩 개념과 GAN의 적대적 학습 메커니즘을 결합하여, 잠재 공간의 분포를 임의의 사전 분포(prior)와 일치시키는 새로운 접근법을 제안했습니다.1 VAE가 KL 발산이라는 명시적인 수식을 통해 잠재 분포를 정규화하는 반면, AAE는 판별자 네트워크를 도입하여 잠재 코드의 집계된 사후 분포(aggregated posterior) $q(z) \= \\int\_x q(z|x)p\_d(x)dx$가 목표 사전 분포 $p(z)$와 구분되지 않도록 학습시킵니다.

AAE의 구조는 표준 오토인코더에 적대적 네트워크가 결합된 형태입니다.

1. **재구성 단계:** 오토인코더가 입력 $x$를 잠재 코드 $z$로 인코딩하고 다시 $x$로 디코딩하여 재구성 손실을 최소화합니다.  
2. **정규화 단계:** 적대적 네트워크의 판별자는 실제 사전 분포 샘플과 인코더가 생성한 잠재 코드를 구별하려 하고, 인코더(생성자 역할)는 판별자를 속이도록 업데이트됩니다.1

이 방식은 잠재 공간에 "구멍(holes)"이 없는 꽉 찬 분포를 형성하도록 유도하여, VAE보다 더 선명하고 현실적인 샘플 생성을 가능하게 합니다.

### **4.2 다양한 응용과 성능 평가**

AAE는 단순한 생성 모델링을 넘어 다양한 응용 분야에서 그 유용성을 입증했습니다.

* **준지도 학습(Semi-Supervised Learning):** MNIST와 SVHN 데이터셋에서 라벨 정보를 잠재 변수의 일부로 취급하여 분류 성능을 향상시켰습니다. AAE는 적은 양의 라벨 데이터만으로도 높은 분류 정확도를 달성했습니다.1  
* **스타일과 콘텐츠 분리:** 이미지의 스타일 정보와 라벨 정보를 서로 다른 잠재 변수로 분리하여 학습함으로써, 특정 스타일을 유지하면서 숫자만 바꾸거나, 숫자를 유지하면서 스타일만 바꾸는 제어 가능한 생성을 구현했습니다.  
* **비지도 클러스터링:** 라벨 정보 없이도 데이터의 내재된 클래스 구조를 학습하여 비지도 클러스터링을 수행할 수 있습니다. MNIST 데이터셋에서 16개 또는 30개의 클러스터로 학습했을 때, 4.10%에서 9.55%의 낮은 에러율을 기록했습니다.1  
* **차원 축소 및 시각화:** AAE는 고차원 데이터를 저차원으로 매핑하면서도 데이터 클러스터 간의 분리네를 명확히 유지하여 우수한 시각화 성능을 보였습니다.

AAE는 VAE와 달리 사전 분포의 정확한 확률 밀도 함수를 알 필요 없이 샘플링만 가능하다면 어떠한 복잡한 분포(예: Swiss Roll 분포, 혼합 가우시안)도 잠재 공간에 부과할 수 있다는 강력한 유연성을 제공합니다.1

## **5\. 점수 기반 생성 모델링 (Score-Based Generative Modeling)**

### **5.1 점수 매칭과 랑주뱅 동역학의 결합**

점수 기반 생성 모델링은 데이터 분포의 로그 밀도 함수의 기울기, 즉 점수(score, $\\nabla\_x \\log p(x)$)를 직접 추정하고 이를 활용하여 샘플링을 수행하는 혁신적인 패러다임입니다.1 이 접근법은 정규화 상수(normalization constant)를 계산할 필요가 없다는 점에서 기존 우도 기반 모델의 난점을 우회합니다. 모델은 점수 매칭(Score Matching) 목적 함수를 통해 데이터의 점수 함수를 근사하는 점수 네트워크 $s\_\\theta(x)$를 학습합니다.

샘플링 과정에서는 랑주뱅 동역학(Langevin Dynamics)을 사용합니다. 이는 임의의 초기 노이즈 상태에서 시작하여 점수 함수가 가리키는 방향(데이터 밀도가 높은 방향)으로 점진적으로 이동하며 샘플을 정제하는 과정입니다:

$$\\tilde{x}\_{t} \= \\tilde{x}\_{t-1} \+ \\frac{\\epsilon}{2} \\nabla\_x \\log p(\\tilde{x}\_{t-1}) \+ \\sqrt{\\epsilon} z\_t $$

여기서 $\\epsilon$은 단계 크기이고 $z\_t \\sim \\mathcal{N}(0, I)$입니다. $\\epsilon \\to 0$이고 $T \\to \\infty$일 때, 이 과정은 데이터 분포 $p(x)$에서 추출된 샘플로 수렴함이 보장됩니다.1

### **5.2 매니폴드 가설과 저밀도 영역의 난제**

그러나 단순한 점수 매칭과 랑주뱅 동역학의 결합은 두 가지 근본적인 문제에 직면합니다.

1. **매니폴드 가설(Manifold Hypothesis):** 실제 고차원 데이터는 전체 공간이 아닌 저차원 매니폴드에 집중되어 있는 경우가 많습니다. 이 경우 주변 공간(ambient space)에서의 점수가 정의되지 않아 점수 매칭이 일관된 추정량을 제공하지 못합니다.1 실험적으로 노이즈를 추가하지 않은 데이터에 대해 점수 매칭을 수행하면 손실 함수가 수렴하지 않고 불규칙하게 변동하는 현상이 관찰되었습니다.  
2. **저밀도 영역에서의 추정 및 혼합 문제:** 데이터 밀도가 낮은 영역에서는 훈련 데이터가 부족하여 점수 추정이 부정확해집니다. 또한, 랑주뱅 동역학이 서로 떨어진 데이터 모드(mode) 사이를 이동하려면 저밀도 영역을 통과해야 하는데, 부정확한 점수 정보로 인해 혼합(mixing) 속도가 매우 느려져 올바른 분포로 수렴하지 못할 수 있습니다.1

### **5.3 노이즈 조건부 점수 네트워크(NCSN)와 어닐링 랑주뱅 동역학**

이러한 문제를 해결하기 위해 Song & Ermon은 데이터를 다양한 수준의 가우시안 노이즈로 섭동(perturb)시키는 전략을 제안했습니다.1 여러 스케일의 노이즈 레벨 $\\{\\sigma\_i\\}\_{i=1}^L$ (여기서 $\\sigma\_1 \> \\sigma\_2 \> \\dots \> \\sigma\_L$)을 설정하고, 모든 노이즈 레벨에 대해 단일 조건부 점수 네트워크 $s\_\\theta(x, \\sigma)$를 학습합니다. 큰 노이즈($\\sigma\_1$)는 데이터 분포를 널리 퍼뜨려 저밀도 영역을 채우고 매니폴드 문제를 완화하며, 작은 노이즈($\\sigma\_L$)는 원본 데이터 분포에 근접합니다.

샘플링 시에는 \*\*어닐링 랑주뱅 동역학(Annealed Langevin Dynamics)\*\*을 사용합니다. 가장 큰 노이즈 레벨 $\\sigma\_1$에서 시작하여 샘플링을 수행하고, 점진적으로 노이즈 레벨을 줄여가며 이전 단계의 샘플을 초기값으로 사용합니다. 이는 시뮬레이티드 어닐링(Simulated Annealing)과 유사하게 작동하여, 모델이 초기에 넓은 영역을 탐색하여 모드 간 이동을 원활하게 하고, 후반부에는 세밀한 구조를 다듬어 고품질 샘플을 생성하게 합니다.1

이 모델(NCSN)은 CIFAR-10 데이터셋에서 Inception Score 8.87을 기록하며, 당시 비조건부 생성 모델 중 최고 수준의 성능을 달성했습니다. 또한, 이미지 인페인팅 실험을 통해 모델이 데이터의 의미론적 표현을 효과적으로 학습했음을 보여주었습니다.1

## **6\. 디노이징 확산 확률 모델 (DDPM): 비평형 열역학의 응용**

### **6.1 확산 과정과 역과정의 정의**

디노이징 확산 확률 모델(DDPM)은 비평형 열역학(nonequilibrium thermodynamics)에서 영감을 받아, 데이터에 서서히 노이즈를 추가하여 구조를 파괴하는 확산 과정(forward process)과 이를 역전시켜 노이즈로부터 데이터를 복원하는 역과정(reverse process)을 학습하는 모델입니다.1

* 확산 과정 (Forward Process): 데이터 $x\_0$에 시간 $t$에 따라 점진적으로 가우시안 노이즈를 주입하는 마르코프 체인입니다. 노이즈 스케줄 $\\beta\_t$에 따라 다음과 같이 정의됩니다:

  $$q(x\_t|x\_{t-1}) \= \\mathcal{N}(x\_t; \\sqrt{1-\\beta\_t}x\_{t-1}, \\beta\_t I)$$

  충분히 많은 단계 $T$가 지나면 $x\_T$는 등방성 가우시안 분포 $\\mathcal{N}(0, I)$에 근사하게 됩니다.  
* 역과정 (Reverse Process): 학습된 모델 파라미터 $\\theta$를 사용하여 $p(x\_T)$에서 시작하여 $x\_0$를 복원하는 과정입니다. 작은 $\\beta\_t$에 대해 역과정 또한 가우시안 분포로 근사될 수 있습니다:

  $$p\_\\theta(x\_{t-1}|x\_t) \= \\mathcal{N}(x\_{t-1}; \\mu\_\\theta(x\_t, t), \\Sigma\_\\theta(x\_t, t))$$

### **6.2 훈련 목표와 $L\_{simple}$**

DDPM의 훈련은 음의 로그 우도의 변분 하한을 최소화하는 것으로 시작하지만, 수학적 재구성을 통해 각 타임스텝 $t$에서의 디노이징 점수 매칭(denoising score matching)과 유사한 형태를 띠게 됩니다. 저자들은 복잡한 가중치 항을 제거한 단순화된 손실 함수 $L\_{simple}$이 실제 샘플 품질 향상에 더 효과적임을 발견했습니다 1:

$$ L\_{simple}(\\theta) \= \\mathbb{E}\_{t, x\_0, \\epsilon} \[\\|\\epsilon \- \\epsilon\_\\theta(\\sqrt{\\bar{\\alpha}\_t}x\_0 \+ \\sqrt{1-\\bar{\\alpha}\_t}\\epsilon, t)\\|^2\] $$

여기서 $\\epsilon\_\\theta$는 노이즈 예측 네트워크입니다. 이 식은 모델이 타임스텝 $t$에서 입력에 추가된 노이즈 $\\epsilon$을 예측하도록 학습됨을 의미합니다. 이는 NCSN이 노이즈가 섞인 데이터의 점수(그라디언트)를 추정하는 것과 본질적으로 동일한 작업입니다. 실제로 $\\epsilon\_\\theta$와 점수 함수 사이에는 비례 관계가 성립합니다.1

### **6.3 점진적 압축 및 자기회귀적 해석**

DDPM은 CIFAR-10 및 LSUN 데이터셋에서 최신 GAN과 비교할 만한, 혹은 더 우수한 샘플 품질(FID 3.17)을 달성했습니다.1 흥미로운 점은 확산 모델의 샘플링 과정이 점진적인 손실 압축 해제(progressive lossy decompression) 과정으로 해석될 수 있다는 것입니다. $x\_T$에서 $x\_0$로 가는 과정은 거친 이미지 특징(coarse features)부터 시작하여 점차 미세한 디테일(fine details)을 추가해 나가는 과정과 같습니다. 이는 일반화된 비트 순서를 가진 자기회귀(autoregressive) 디코딩과 유사하며, 정보 이론적 관점에서 확산 모델이 훌륭한 손실 압축기(lossy compressor)로서의 귀납적 편향(inductive bias)을 가지고 있음을 시사합니다.1

실험 결과에 따르면, 모델의 무손실 코드 길이(lossless codelength)의 대부분은 지각적으로 감지하기 어려운 이미지 세부 정보를 묘사하는 데 사용됩니다. 이는 확산 모델이 데이터의 개념적 압축(conceptual compression)을 수행하고 있음을 보여주는 강력한 증거입니다.

## **7\. 확률 미분 방정식(SDE)을 통한 통합 프레임워크**

### **7.1 이산적 단계에서 연속적 시간으로**

최근 연구는 SMLD(NCSN)와 DDPM을 확률 미분 방정식(SDE)이라는 하나의 통합된 프레임워크로 재해석함으로써 이론적 깊이를 더했습니다.1 유한한 수의 노이즈 레벨을 사용하는 대신, 연속적인 시간 변수 $t \\in$에 따라 데이터 분포를 사전 분포로 변환하는 확산 과정을 SDE로 모델링합니다:

$$dx \= f(x, t)dt \+ g(t)dw$$

여기서 $f(x, t)$는 표류(drift) 계수, $g(t)$는 확산(diffusion) 계수, $w$는 브라운 운동(Brownian motion)입니다. 이 SDE는 데이터 $x(0)$를 시간 $T$에서 사전 분포 $p\_T$로 확산시킵니다.  
이 프레임워크에서 SMLD와 DDPM은 서로 다른 SDE의 이산화(discretization)로 해석됩니다 1:

* Variance Exploding (VE) SDE: SMLD(NCSN)에 해당합니다. 시간이 지날수록 노이즈의 분산이 발산하는 형태를 띱니다.

  $$dx \= \\sqrt{\\frac{d\[\\sigma^2(t)\]}{dt}} dw$$  
* Variance Preserving (VP) SDE: DDPM에 해당합니다. 시간이 지나도 분산이 일정하게 유지되도록(주로 1로) 설계되었습니다.

  $$dx \= \-\\frac{1}{2}\\beta(t)x dt \+ \\sqrt{\\beta(t)} dw$$

  또한, 저자들은 우도(likelihood) 성능을 향상시키기 위해 VP SDE의 분산을 조절한 새로운 sub-VP SDE를 제안했습니다.

### **7.2 역시간 SDE와 점수 기반 생성**

Anderson의 이론에 따르면, 확산 과정의 역과정 또한 SDE로 표현될 수 있으며, 이때 데이터의 점수 함수 $\\nabla\_x \\log p\_t(x)$가 필요합니다 1:

$$dx \= \[f(x, t) \- g(t)^2 \\nabla\_x \\log p\_t(x)\]dt \+ g(t)d\\bar{w}$$

이 식은 생성 과정의 핵심입니다. 시간 의존적 점수 기반 모델 $s\_\\theta(x, t)$를 훈련하여 $\\nabla\_x \\log p\_t(x)$를 추정하면, 수치적 SDE 솔버(예: Euler-Maruyama)를 사용하여 역시간 SDE를 시뮬레이션함으로써 사전 분포의 노이즈로부터 데이터 샘플을 생성할 수 있습니다. 이는 기존의 이산적 샘플링 방식을 일반화한 것입니다.

### **7.3 확률 흐름 ODE (Probability Flow ODE)**

SDE 프레임워크의 또 다른 강력한 점은 모든 확산 과정에 대해 동일한 주변 확률 밀도(marginal probability density) 궤적을 따르는 결정론적 과정인 '확률 흐름 ODE'가 존재한다는 것입니다 1:

$$dx \= \[f(x, t) \- \\frac{1}{2}g(t)^2 \\nabla\_x \\log p\_t(x)\]dt$$

이 ODE는 무작위성 없이 데이터와 잠재 공간 사이를 매핑할 수 있게 해줍니다. 이를 통해 다음과 같은 이점을 얻을 수 있습니다:

1. **정확한 우도 계산:** 신경망 ODE(Neural ODE)와의 연결을 통해 변분 하한이 아닌 정확한 로그 우도를 계산할 수 있습니다. 실험 결과, sub-VP SDE를 사용한 모델은 CIFAR-10에서 2.99 bits/dim이라는 경쟁력 있는 로그 우도를 달성하여 기존 DDPM의 한계를 뛰어넘었습니다.1  
2. **잠재 공간 조작:** GAN이나 Flow 모델처럼 잠재 공간에서의 인코딩 및 조작(예: 이미지 보간, 온도 조절)이 가능해집니다.  
3. **효율적인 샘플링:** 적응형 스텝 사이즈(adaptive step size)를 사용하는 블랙박스 ODE 솔버를 활용하여 샘플링 속도와 정확도 간의 트레이드오프를 조절할 수 있습니다.

### **7.4 예측자-수정자(Predictor-Corrector) 샘플러**

SDE 프레임워크는 새롭고 강력한 샘플링 알고리즘인 예측자-수정자(PC) 샘플러를 도입했습니다.1

* **예측자(Predictor):** 수치적 SDE 솔버를 사용하여 현재 샘플을 다음 타임스텝으로 이동시킵니다.  
* **수정자(Corrector):** 점수 기반 MCMC(예: 랑주뱅 동역학)를 사용하여 예측된 샘플의 분포를 해당 시점의 실제 점수 함수를 이용해 보정합니다.

이 방법은 수치적 적분 과정에서 발생하는 오차를 MCMC 단계가 수정해 줌으로써 샘플의 품질을 비약적으로 향상시킵니다. 실험 결과, PC 샘플러를 적용한 모델은 CIFAR-10에서 FID 2.20, Inception Score 9.89라는 기록적인 성능을 달성했으며, $1024 \\times 1024$ 해상도의 고화질 이미지 생성에도 성공했습니다.1

### **7.5 역문제와 제어 가능한 생성 (Controllable Generation)**

SDE 프레임워크는 훈련 중에는 없었던 정보를 조건으로 주어 생성 과정을 제어할 수 있는 강력한 기능을 제공합니다.1 이는 역시간 SDE에 조건부 점수 항을 추가함으로써 가능해집니다:

$$\\nabla\_x \\log p\_t(x|y) \= \\nabla\_x \\log p\_t(x) \+ \\nabla\_x \\log p\_t(y|x)$$

여기서 첫 번째 항은 무조건부 점수 모델이 제공하고, 두 번째 항(분류기 또는 측정 모델)은 조건 $y$에 대한 정보를 제공합니다. 이를 통해 다음과 같은 역문제(inverse problems)를 별도의 재학습 없이 단일 무조건부 점수 모델만을 사용하여 해결할 수 있습니다:

* **클래스 조건부 생성:** 분류기(classifier)의 그라디언트를 사용하여 특정 클래스의 이미지를 생성합니다.  
* **이미지 인페인팅(Inpainting):** 손실된 이미지 영역을 주변 정보를 바탕으로 복원합니다. 마스크된 영역과 원본 이미지 사이의 관계를 조건부 점수로 모델링하여 해결합니다.  
* **컬러라이제이션(Colorization):** 흑백 이미지를 컬러로 변환합니다. 회색조 이미지를 조건으로 주어 색상 채널을 생성합니다.

이러한 접근법은 범용적인 생성 모델 하나로 다양한 응용 작업을 수행할 수 있게 하여 모델의 활용도를 극대화합니다.

## **8\. 모델별 성능 비교 및 종합 분석**

다음 표는 주요 모델들의 CIFAR-10 데이터셋에 대한 성능 지표를 요약한 것입니다.

| 모델 | Inception Score (IS) | FID Score | NLL (bits/dim) | 비고 |
| :---- | :---- | :---- | :---- | :---- |
| **GAN (SNGAN)** 1 | 8.22 | 21.7 | \- | 적대적 학습, 우도 계산 불가 |
| **VAE (PixelIQN)** 1 | 5.29 | 49.46 | \- | 변분 추론, 흐릿한 생성 경향 |
| **NCSN (SMLD)** 1 | 8.87 | 25.32 | \- | 점수 매칭, 어닐링 랑주뱅 |
| **DDPM ($L\_{simple}$)** 1 | 9.46 | 3.17 | $\\le 3.75$ | 확산 모델, 고품질 생성 |
| **NCSN++ cont. (VE)** 1 | 9.83 | 2.38 | \- | SDE 프레임워크, PC 샘플러 |
| **DDPM++ cont. (sub-VP)** 1 | 9.57 | 2.41 | **2.99** | SDE 프레임워크, ODE 우도 계산 |

* **샘플 품질:** GAN은 오랫동안 최고 수준의 샘플 품질을 자랑했으나, 최신 연구 결과(DDPM, NCSN++)에 따르면 점수 기반/확산 모델이 GAN을 능가하거나 대등한 수준의 FID 및 IS 점수를 달성하고 있습니다. 특히 DDPM과 NCSN++는 2\~3점대의 극히 낮은 FID를 기록하며 사실적인 이미지 생성 능력을 입증했습니다.  
* **우도(Likelihood):** VAE와 초기 우도 기반 모델들이 강점을 가졌던 영역이지만, 확률 흐름 ODE를 도입한 SDE 기반 모델들이 2.99 bits/dim과 같은 경쟁력 있는 수치를 기록하며 우도 추정 능력 또한 크게 향상되었습니다. GAN은 우도 계산이 불가능하다는 단점이 여전합니다.  
* **훈련 안정성 및 속도:** GAN은 훈련이 매우 불안정하고 모드 붕괴 위험이 높습니다. 반면 점수 기반 모델과 VAE는 훈련이 안정적입니다. 하지만 샘플링 속도 면에서는 GAN과 VAE가 한 번의 패스로 생성하는 반면, 확산 모델은 수천 번의 반복이 필요하여 매우 느립니다. 이를 개선하기 위한 ODE 솔버 적용 등의 연구가 활발합니다.1

## **9\. 결론 및 향후 전망**

본 연구 보고서는 생성 모델링의 발전 과정을 GAN, VAE, AAE에서부터 최신 점수 기반 생성 모델 및 DDPM까지 포괄적으로 분석했습니다. 초기 모델들이 겪었던 학습의 불안정성이나 생성 품질의 한계는 점수 매칭과 확산 과정이라는 새로운 패러다임을 통해 획기적으로 개선되었습니다. 특히 확률 미분 방정식을 이용한 통합 프레임워크는 이산적인 노이즈 레벨을 다루던 기존 연구들을 연속 시간 도메인으로 확장하여 이론적 완전성을 제공했습니다. 이는 샘플링 알고리즘의 유연성을 높이고(PC 샘플러), 정확한 우도 계산을 가능하게 하며(확률 흐름 ODE), 인페인팅이나 컬러라이제이션과 같은 역문제를 해결하는 데 있어 일관된 방법론을 제시했다는 점에서 큰 의의가 있습니다.

향후 연구는 이러한 고성능 모델들의 실용성을 높이기 위해 **샘플링 속도를 획기적으로 개선**하는 방향(예: 증류(distillation), 고속 ODE 솔버)과, 이미지뿐만 아니라 오디오, 비디오, 그래프 등 **더욱 복잡하고 다양한 데이터 양식으로 적용 범위를 확장**하는 방향으로 진행될 것으로 전망됩니다. 또한, SDE 프레임워크가 제공하는 제어 가능한 생성 능력은 의료 영상 복원, 물리 시뮬레이션 등 정밀한 제어가 필요한 과학 및 공학 분야에서 생성 AI의 활용 가능성을 크게 넓힐 것입니다.

#### **참고 자료**

1. 01.Generative Adversarial Nets(GAN).pdf