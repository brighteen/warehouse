## 퀴즈 풀이
### 강화학습이란
순차적 의사결정에서 시행착오를 통해 누적 보상을 최대화하도록 행동을 학습한다. 이때 순차적 의사결정은 에이전트가 시간의 흐름에 따라 연속적으로 행동을 선택하는 환경에서 이루어진다. 또한 에이전트에게 정답이 주어지지 않는 대신, 스스로 다양한 행동을 시도해보고 그 결과로 주어지는 보상을 통해 어떤 행동이 좋은지 나쁜지 학습한다. 강화학습의 유일한 목표는 당장 눈앞의 보상이 아닌 미래에 받을 모든 보상의 총합(누적 보상)을 최대로 만드는 행동 방식을 학습한다.

보상(reward)은 강호학습 에이전트에게 방금 한 행동이 얼마나 좋고 나쁜지를 알려주는 평가신호이다. 이 신호는 어떤 행동을 하라고 지시하는게 아닌 스칼라 값이다.

에이전트(agent)란 강화학습에서 학습의 주체이자 정책(policy)을 통해 행동(actions)을 결정하고 환경(environment)과 상호작용하여 학습한다.

### MDP(Markov Decision Process, 마르코프 결정 과정)에서 전이확률이란
현재 상태(s)에서 특정 행동(a)를 했을 때, 그 결과로 나타날 수 있는 모든 가능한 다음 상태(s')들의 확률값을 전부 더하면 그 총합은 반드시 1이 되어야 하는데 이를 수식으로 표현하면 다음과 같다.
∑ P(s'|s, a) = 1 (for all s ∈ S, a ∈ A, s' ∈ S) <- 확률의 기본적인 공리
확률의 총합은 모든 a에 대해 합이 1이 아닌 다음 상태 (s')에 대해 구해야 한다.
마르코프 성질에 의해 MDP의 전이 확률은 현재 상태(s)와 행동(a)에만 의존하며, 과거의 상태나 행동에는 의존하지 않고, 표준적인 MDP는 고정된 확률분포를 가정하기에 환경의 규칙(전이 확률)은 학습 도중에 변하지 않는다.

Markov Property에서 다음 상태 분포는 현재 상태(s시점에서의 행동 a)에만 의존한다.(기억 없음 = 다음 상태를 예측하는데 필요한 정보가 현재 상태에 모두 포함되어 있음) 따라서 MDP에서는 과거의 상태나 행동이 다음 상태에 영향을 미치지 않는다. 또한 모든 전이는 결정적 전이, 확률적 전이 모두 적용될 수 있음.

정책 $\pi(a|s)$의 의미는 현재 상태 s에서 행동 a를 선택할 조건부 확률임. 즉, 에이전트가 특정 상황에서 어떤 행동을 할지 결정하는 규칙을 정의한다.

MDP의 구성 요소는 다음과 같다.
상태 State: 에이전트가 관찰할 수 있는 모든 가능한 상황의 집합.
행동 Action: 각 상태에서 에이전트가 선택할 수 있는 모든 가능한 행동의 집합.
상태 전이 확률 State Transition Probability: 현재 상태(s)와 행동(a)에 따라 다음 상태(s')로 전이될 확률을 나타내는 함수.(P(s'|s, a))
보상 함수 Reward Function: 현재 상태(s)와 행동(a)에 따라 즉시 받는 보상(의 기댓값)을 나타내는 함수.(R(s, a))
감가율 Discount Factor: 미래 보상의 현재 가치에 대한 중요도를 나타내는 값(0과 1 사이의 실수). 감가율이 높을수록 먼 미래의 보상도 현재 가치에 큰 영향을 미친다.(γ)

### 할인 리턴 $G_t$
현재 시점(t)이후로 미래에 받을 모든 보상(R)들을 감가율($\gamma$)를 적용하여 합산한 값.
$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$
감가율은 미래에 받을 보상을 현재 가치로 할인하여 장단기 보상의 균형을 맞추고 수렴성을 보장함.

### 보상함수 R이란
현재 상태(s)와 행동(a)에 대한 다음 보상의 기댓값으로 정의됨.
$$R_s^a = E[R_{t+1} | S_t = s, A_t = a]$$

### 상태 가치 함수 $v_\pi(s)$
$$v_\pi(s) = E_\pi[G_t |S_t=s]$$
'기대 리턴' 관점에서 정책 $\pi$를 따를 때, 현재 상태 $s$에서 시작하여 앞으로 얻을 **기대 누적 보상(기대 리턴)** 의 크기를 의미함.

### 행동 가치 함수 $q_\pi(s,a)$
$$q_\pi(s,a) = E_\pi[G_t |S_t=s, A_t=a]$$
'기대 리턴' 관점에서 정책 $\pi$를 따를 때, 현재 상태 $s$에서 행동 $a$를 수행한 이후 얻게 될 **기대 누적 보상(기대 리턴)** 을 의미함.

>기대 리턴: 앞으로 받을 보상의 총합(return)의 기댓값(평균)을 의미.

### Precidtion과 Control task의 차이
prediction은 주어진(이미 알고 있는) 정책 $\pi$에 대한 상태 가치 함수 $v_\pi(s)$ 또는 행동 가치 함수 $q_\pi(s,a)$를 추정하는 문제이다. 즉, 특정 정책이 주어졌을 때 그 정책을 따랐을 때의 기대 누적 보상을 평가하는 것이다.

control은 최적의 정책 $\pi^*$를 찾는 문제이다. 즉, 가능한 모든 정책 중에서 누적 보상을 최대화하는 정책을 찾는 것이다.

## Markov Process(Property)

### MP(markov Process, 마르코프 과정)
$MP = F(S, P) (S={s_1, s_2, ..., s_n}, P_{ss'}=P[S_{t+1}=s'|S_t=s])$
미래는 오로지 현재에 의해 결정됨.
상태와 전이확률만 존재.

### MRP(markov Reward Process, 마르코프 보상 과정)
$MRP = F(S, P, R, \gamma) (S={s_1, s_2, ..., s_n}, P_{ss'}=P[S_{t+1}=s'|S_t=s], R_s=E[R_{t+1}|S_t=s], \gamma \in [0,1])$
MP에 보상이 추가됨.
상태, 전이확률, 보상함수, 감가율 존재.(여전히 에이전트의 행동이나 선택은 없음)

Return(리턴):
$G_t$는 현재 시점(t) 이후로 미래에 받을 모든 보상(R)들을 감가율($\gamma$)를 적용하여 합산한 값이다.
$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$
리턴의 반환값을 재귀적으로 표현하면 다음과 같다.
$G_t = R_{t+1} + \gamma G_{t+1}$
상태(s)에서의 가치는 리턴의 기댓값으로 정의하는데 이는 무한한 에피소드의 리턴을 다 확인할 수 없기 때문에 평균적인 기대치를 사용하는 것이 합리적임.
$v_{\pi}(s) = E[G_t | S_t=s]$

MP와 MRP는 상태 변화가 자동(의사결정 X)으로 이루어지기 때문에 순차적의사결정 문제를 다루기 어려움.

### MDP(markov Decision Process, 마르코프 결정 과정)
$MDP = F(S, A, P, R, \gamma) (S={s_1, s_2, ..., s_n}, A={a_1, a_2, ..., a_m}, P_{ss'}^a=P[S_{t+1}=s'|S_t=s, A_t=a], R_s^a=E[R_{t+1}|S_t=s, A_t=a], \gamma \in [0,1])$
MRP에 행동이 추가됨.
에이전트가 상태(s)에서 행동(a)를 선택할 수 있음. 따라서 전이 확률 P와 보상 R이 행동a에 대한 조건부 확률로 표현됨.
P(s'|s, a): 상태s에서 행동a를 했을 때 다음 상태s'로 전이될 확률
R(s, a): 상태s에서 행동a를 했을 때 받는 보상의 기댓값
MDP의 목표는 최적의 정책$\pi^*$를 찾아 누적 보상을 최대화하는 것임.

#### 정책함수 $\pi(a|s)$:
정책함수 $\pi(a|s)$는 상태s에서 행동a를 선택할 조건부 확률을 나타낸다. 즉, 에이전트가 특정 상황에서 어떤 행동을 할지 결정하는 규칙을 정의한다.
정책에 따른 전이확률: 정책 $\pi$를 따를 때, 현재 상태 s에서 다음 상태 s'로 이동할 종합 확률
$$P_{s,s'}^{\pi} = \sum_{a} \pi(a|s) P_{s,s'}^a,$$
모든 가능한 행동(a)에 대해 상태(s)에서 행동(a)를 선택할 확률($\pi(a|s)$)과 그 행동(a)을 했을 때 상태(s')로 전이될 확률($P_{s,s'}^a$)을 곱한 후 합산함.

정책에 따른 보상함수: 정책 $\pi$를 따를 때, 현재 상태 s에서 기대할 수 있는 평균 보상
$$R_s^{\pi} = \sum_{a} \pi(a|s) R_s^a$$
모든 가능한 행동(a)에 대해 상태(s)에서 행동(a)을 선택할 확률($\pi(a|s)$)과 그 행동(a)을 했을 때 받는 보상($R_s^a$)을 곱한 후 합산함.

#### 상태가치함수 $v_\pi(s)$:
$$v_\pi(s) = E_\pi[G_t |S_t=s]$$
$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} $$
이때 정책$\pi$가 given되었다는 의미는 에이전트가 앞으로 행동을 선택할 때 정책$\pi$를 따른다는 의미이다.