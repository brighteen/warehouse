## 퀴즈 풀이

### 강화학습이란

순차적 의사결정에서 **시행착오**를 통해 **누적 보상을 최대화**하도록 행동을 학습합니다.

* **순차적 의사결정**: 에이전트가 시간의 흐름에 따라 연속적으로 행동을 선택하는 환경에서 이루어집니다.
* **학습 방식**: 에이전트에게 정답이 주어지지 않고, 스스로 다양한 행동을 시도하며 그 결과로 주어지는 보상을 통해 행동의 좋고 나쁨을 학습합니다.
* **목표**: 당장 눈앞의 보상이 아닌, 미래에 받을 모든 보상의 총합(누적 보상)을 최대로 만드는 행동 방식을 학습합니다.

### 핵심 구성 요소

* **보상 (Reward)**: 에이전트에게 방금 한 행동이 얼마나 좋고 나쁜지를 알려주는 **평가 신호**입니다. 이 신호는 '어떤 행동을 하라'고 지시하는 것이 아닌, **스칼라 값**입니다.
* **에이전트 (Agent)**: 강화학습의 주체이자, **정책(Policy)** 을 통해 행동(Actions)을 결정하고 환경(Environment)과 상호작용하며 학습합니다.

### MDP와 마르코프 성질

**MDP (마르코프 결정 과정) 전이 확률**
* 현재 상태($s$)에서 특정 행동($a$)을 했을 때, 그 결과로 나타날 수 있는 모든 가능한 다음 상태($s'$)들의 확률값을 전부 더하면 그 총합은 반드시 1이 되어야 합니다. 이는 확률의 기본적인 공리입니다.
* **수식**: $\sum_{s' \in S} P(s'|s, a) = 1$ (for all $s \in S, a \in A$)
* **주의**: 확률의 총합은 '다음 상태($s'$)'에 대해 구해야 하며, '모든 행동($a$)'에 대해 합이 1이 되는 것이 아닙니다.
* **가정**: 표준적인 MDP는 고정된 확률분포(stationary)를 가정하므로, 환경의 규칙(전이 확률)은 학습 도중에 변하지 않습니다.

**마르코프 성질 (Markov Property)**
* **정의**: 다음 상태 분포는 오직 **현재 상태($s$)** 와 **현재 행동($a$)** 에만 의존합니다. (이를 "기억이 없다(memoryless)"고 표현합니다.)
* **의미**: 다음 상태를 예측하는 데 필요한 모든 정보가 현재 상태에 포함되어 있으므로, 과거의 상태나 행동은 다음 상태에 영향을 미치지 않습니다.
* **전이**: 모든 전이는 결정적(deterministic) 전이와 확률적(stochastic) 전이 모두에 적용될 수 있습니다.

### 정책과 MDP 구성 요소

**정책 (Policy) $\pi(a|s)$**
* **정의**: 현재 상태 $s$에서 행동 $a$를 선택할 **조건부 확률**입니다.
* **역할**: 에이전트가 특정 상황에서 어떤 행동을 할지 결정하는 규칙을 정의합니다.

**MDP의 5가지 구성 요소**
1.  **상태 (State, $S$)**: 에이전트가 관찰할 수 있는 모든 가능한 상황의 집합.
2.  **행동 (Action, $A$)**: 각 상태에서 에이전트가 선택할 수 있는 모든 가능한 행동의 집합.
3.  **상태 전이 확률 (Transition Probability, $P$)**: 현재 상태($s$)와 행동($a$)에 따라 다음 상태($s'$)로 전이될 확률. ($P(s'|s, a)$)
4.  **보상 함수 (Reward Function, $R$)**: 현재 상태($s$)와 행동($a$)에 따라 즉시 받는 보상(의 기댓값). ($R(s, a)$)
5.  **감가율 (Discount Factor, $\gamma$)**: 미래 보상의 현재 가치에 대한 중요도를 나타내는 값 (0과 1 사이). 감가율이 높을수록(1에 가까울수록) 먼 미래의 보상도 현재 가치에 큰 영향을 미칩니다.

### 리턴과 가치 함수

**할인 리턴 (Discounted Return, $G_t$)**
* 현재 시점($t$) 이후로 미래에 받을 모든 보상($R$)들을 감가율($\gamma$)을 적용하여 합산한 값입니다.
* **수식**: $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$
* **감가율의 역할**: 미래에 받을 보상을 현재 가치로 할인하여 장단기 보상의 균형을 맞추고, (무한한 경우) 합이 수렴하도록 보장합니다.

**보상 함수 (Reward Function, $R$)**
* 현재 상태($s$)와 행동($a$)에 대한 다음 보상의 기댓값으로 정의됩니다.
* **수식**: $$R_s^a = E[R_{t+1} | S_t = s, A_t = a]$$

**상태 가치 함수 ($v_\pi(s)$)**
* **수식**: $$v_\pi(s) = E_\pi[G_t | S_t = s]$$
* **의미**: 정책 $\pi$를 따를 때, 현재 상태 $s$에서 시작하여 앞으로 얻을 **기대 누적 보상(기대 리턴)** 의 크기를 의미합니다.

**행동 가치 함수 ($q_\pi(s,a)$)**
* **수식**: $$q_\pi(s,a) = E_\pi[G_t | S_t = s, A_t = a]$$
* **의미**: 정책 $\pi$를 따를 때, 현재 상태 $s$에서 행동 $a$를 수행한 이후 얻게 될 **기대 누적 보상(기대 리턴)** 을 의미합니다.

> **기대 리턴 (Expected Return)**: 앞으로 받을 보상의 총합(Return)의 기댓값(평균)을 의미합니다.

### Prediction vs Control

* **Prediction (예측)**: **주어진(이미 알고 있는) 정책 $\pi$** 에 대한 가치 함수($v_\pi(s)$ 또는 $q_\pi(s,a)$)를 **추정(평가)** 하는 문제입니다.
* **Control (제어)**: 가능한 모든 정책 중에서 누적 보상을 최대화하는 **최적 정책 $\pi^*$** 을 **찾는** 문제입니다.

---

## Markov Process(Property)

### 1. MP (Markov Process, 마르코프 과정)

* **구성**: $(S, P)$
    * $S$: 상태의 집합 $\{s_1, s_2, \dots, s_n\}$
    * $P$: 상태 전이 확률 ($P_{ss'} = P[S_{t+1}=s'|S_t=s]$)
* **특징**: 미래는 오로지 현재에 의해서만 결정되며, **상태**와 **전이 확률**만 존재합니다.

### 2. MRP (Markov Reward Process, 마르코프 보상 과정)

* **구성**: $(S, P, R, \gamma)$
    * $S$: 상태의 집합
    * $P$: 상태 전이 확률 ($P_{ss'} = P[S_{t+1}=s'|S_t=s]$)
    * $R$: 보상 함수 ($R_s = E[R_{t+1}|S_t=s]$)
    * $\gamma$: 감가율 ($\gamma \in [0,1]$)
* **특징**: MP에 **보상(R)** 과 **감가율($\gamma$)** 이 추가되었습니다. (여전히 에이전트의 행동이나 선택은 없음)
* **리턴 (Return, $G_t$)**: 현재 시점($t$) 이후로 미래에 받을 모든 보상들을 감가율($\gamma$)을 적용하여 합산한 값입니다.
    * **수식**: $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$
    * **재귀적 표현**: $G_t = R_{t+1} + \gamma G_{t+1}$
* **가치 (Value, $v_\pi(s)$)**: 상태 $s$에서의 가치는 리턴의 기댓값으로 정의합니다. ($v_\pi(s) = E[G_t | S_t=s]$) 무한한 에피소드의 모든 리턴을 확인할 수 없으므로 평균적인 기대치를 사용하는 것이 합리적입니다.
* **한계**: MP와 MRP는 상태 변화가 자동으로(의사결정 X) 이루어지므로 순차적 의사결정 문제를 다루기 어렵습니다.

### 3. MDP (Markov Decision Process, 마르코프 결정 과정)

* **구성**: $(S, A, P, R, \gamma)$
    * $S$: 상태의 집합
    * $A$: 행동의 집합
    * $P$: 상태 전이 확률 ($P_{ss'}^a = P[S_{t+1}=s'|S_t=s, A_t=a]$)
    * $R$: 보상 함수 ($R_s^a = E[R_{t+1}|S_t=s, A_t=a]$)
    * $\gamma$: 감가율 ($\gamma \in [0,1]$)
* **특징**: MRP에 **행동(A)**이 추가되었습니다. 에이전트가 상태($s$)에서 행동($a$)을 **선택**할 수 있습니다.
* **핵심 변화**: 전이 확률($P$)과 보상($R$)이 에이전트의 행동($a$)에 대한 조건부 함수가 됩니다.
    * $P(s'|s, a)$: 상태 $s$에서 행동 $a$를 했을 때 다음 상태 $s'$로 전이될 확률
    * $R(s, a)$: 상태 $s$에서 행동 $a$를 했을 때 받는 보상의 기댓값
* **목표**: 누적 보상을 최대화하는 **최적의 정책($\pi^*$)** 을 찾는 것입니다.

#### MDP의 핵심 함수

**정책 함수 (Policy, $\pi(a|s)$)**
* **정의**: 상태 $s$에서 행동 $a$를 선택할 조건부 확률입니다. (에이전트의 행동 규칙)
* **정책 적용 시 전이 확률**: 정책 $\pi$를 따를 때, $s$에서 $s'$로 이동할 종합 확률입니다.
    $$P_{s,s'}^{\pi} = \sum_{a} \pi(a|s) P_{s,s'}^a$$
    (의미: $s$에서 $a$를 할 확률($\pi$)과, $a$를 했을 때 $s'$로 갈 확률($P$)을 모든 $a$에 대해 곱하고 더합니다.)
* **정책 적용 시 보상 함수**: 정책 $\pi$를 따를 때, 상태 $s$에서 기대할 수 있는 평균 보상입니다.
    $$R_s^{\pi} = \sum_{a} \pi(a|s) R_s^a$$
    (의미: $s$에서 $a$를 할 확률($\pi$)과, $a$를 했을 때 받는 보상($R$)을 모든 $a$에 대해 곱하고 더합니다.)

**상태 가치 함수 ($v_\pi(s)$)**
* **정의**: 정책 $\pi$를 따른다는(given) 가정 하에, 현재 상태 $s$에서 시작하여 미래에 얻을 리턴($G_t$)의 기댓값입니다.
* **수식**: $v_\pi(s) = E_\pi[G_t | S_t = s]$ (이때 $G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$)

**행동 가치 함수 ($q_\pi(s,a)$)**
* **정의**: 정책 $\pi$를 따를 때, 현재 상태 $s$에서 행동 $a$를 수행한 이후 얻게 될 리턴($G_t$)의 기댓값입니다.
* **수식**: $q_\pi(s,a) = E_\pi[G_t | S_t = s, A_t = a]$
* **활용**: 각 상태에서 가능한 행동들을 모두 평가한 뒤, 가장 가치 있는 행동을 선택하는 방식으로 최적 정책을 도출할 수 있습니다.

#### MDP의 과제 (Tasks)

* **Prediction (예측)**: **주어진** 정책 $\pi$에 대한 가치 함수($v_\pi(s)$ 또는 $q_\pi(s,a)$)를 **추정(평가)**하는 문제입니다.
* **Control (제어)**: 가능한 모든 정책 중에서 누적 보상을 최대화하는 **최적의 정책 $\pi^*$**을 **찾는** 문제입니다.

#### 'MDP가 주어졌을 때'의 의미
"MDP가 주어졌다"는 것은 환경의 모든 규칙, 즉 **$(S, A, P, R, \gamma)$ 5가지 구성 요소를 이미 알고 있다**는 것을 의미합니다.


## 벨만방정식
현재 시점(t)의 가치(value)와 다음 시점(t+1)의 가치 사이의 관계를 정의하는 재귀적인(recursive) 방정식임.
현재 상태의 가치를 알기 위해 미래 상태의 가치를 이용해 표현(참조)함.

이 방정식은 동적 계획법(Dynamic Programming)에 뿌리를 두고 있으며, 복잡한 문제를 더 작은 하위 문제로 나누어 풉니다.

벨만 방정식은 목적에 따라 **기대 방정식**과 **최적 방정식** 두 가지로 나뉩니다.

---

### 1. 벨만 기대 방정식 (Bellman Expectation Equation)

* **목적: Prediction (평가)**
* **개념**: **이미 주어진 특정 정책($\pi$)**을 따랐을 때의 가치를 계산합니다.
* **핵심 연산**: 정책($\pi$)이 확률적으로 행동하므로($\pi(a|s)$), 모든 가능한 행동과 그로 인한 다음 상태를 **기댓값(평균)**으로 계산합니다.

#### 완전한 재귀 방정식 (2단계)

* **상태 가치 ($v_\pi$)**: 현재 상태 $s$의 가치는, 정책($\pi$)에 따라 모든 행동($a$)을 선택할 확률로 가중 평균을 낸 값입니다.

    $v_{\pi}(s)=\sum_{a\in A}\pi(a|s)[r_{s}^{a}+\gamma\sum_{s^{\prime}\in S}p_{ss^{\prime}}^{a}v_{\pi}(s^{\prime})]$ 

* **행동 가치 ($q_\pi$)**: 현재 $(s, a)$의 가치는, (즉시 보상) + (다음 상태 $s'$에서 다시 정책 $\pi$를 따랐을 때의 평균적인 미래 가치)입니다.

    $q_{\pi}(s,a)=r_{s}^{a}+\gamma\sum_{s^{\prime}\in S}p_{ss^{\prime}}^{a}\sum_{a^{\prime}\in A}\pi(a^{\prime}|s^{\prime})q_{\pi}(s^{\prime},a^{\prime})$

---

### 2. 벨만 최적 방정식 (Bellman Optimality Equation)

* **목적: Control (제어)**
* **개념**: 가능한 모든 정책 중에서 **최고의 정책($\pi_*$)**, 즉 **최적 가치($v_*, q_*)**를 찾습니다.
* **핵심 연산**: "평균"을 내는 대신, 모든 가능한 행동($a$) 중에서 가장 높은 가치를 주는 행동을 **최댓값($\max$)** 연산으로 선택합니다.

#### 완전한 재귀 방정식 (2단계)

* **최적 상태 가치 ($v_*$)**: 현재 상태 $s$의 최적 가치는, 가능한 모든 행동($a$) 중에서 **가장 높은 가치**를 주는 행동을 선택했을 때의 값입니다.

    $v_{*}(s)=max_{a}[r_{s}^{a}+\gamma\sum_{s^{\prime}\in S}p_{ss^{\prime}}^{a}v_{*}(s^{\prime})]$

* **최적 행동 가치 ($q_*$)**: 현재 $(s, a)$의 최적 가치는, (즉시 보상) + (다음 상태 $s'$에서 다시 **최적의 행동($\max_{a'}$)을 선택**했을 때의 미래 가치)입니다.

    $q_{*}(s,a)=r_{s}^{a}+\gamma\sum_{s^{\prime}\in S}p_{ss^{\prime}}^{a}max_{a^{\prime}}q_{*}(s^{\prime},a^{\prime})$

---

### 요약 비교

| 구분 | 벨만 기대 방정식 (Expectation) | 벨만 최적 방정식 (Optimality) |
| :--- | :--- | :--- |
| **목적** | **Prediction (평가)**  | **Control (제어)**  |
| **질문** | "이 정책($\pi$)은 얼마나 좋은가?" | "최고의 정책($\pi_*$)은 무엇인가?"  |
| **가치** | $v_\pi(s), q_\pi(s,a)$  | $v_*(s), q_*(s,a)$  |
| **핵심 연산**| **기댓값 (Expectation, $\sum \pi$)** | **최댓값 (Maximum, $\max$)** |