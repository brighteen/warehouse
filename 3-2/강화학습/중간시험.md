### 강화학습이란
순차적 의사결정에서 시행착오를 통해 누적 보상을 최대화하도록 행동을 학습한다. 이때 순차적 의사결정은 에이전트가 시간의 흐름에 따라 연속적으로 행동을 선택하는 환경에서 이루어진다. 또한 에이전트에게 정답이 주어지지 않는 대신, 스스로 다양한 행동을 시도해보고 그 결과로 주어지는 보상을 통해 어떤 행동이 좋은지 나쁜지 학습한다. 강화학습의 유일한 목표는 당장 눈앞의 보상이 아닌 미래에 받을 모든 보상의 총합(누적 보상)을 최대로 만드는 행동 방식을 학습한다.

보상(reward)은 강호학습 에이전트에게 방금 한 행동이 얼마나 좋고 나쁜지를 알려주는 평가신호이다. 이 신호는 어떤 행동을 하라고 지시하는게 아닌 스칼라 값이다.

에이전트(agent)란 강화학습에서 학습의 주체이자 정책(policy)을 통해 행동(actions)을 결정하고 환경과 상호작용하여 학습한다.

### MDP(Markov Decision Process, 마르코프 결정 과정)에서 전이확률이란
현재 상태(s)에서 특정 행동(a)를 했을 때, 그 결과로 나타날 수 있는 모든 가능한 다음 상태(s')들의 확률값을 전부 더하면 그 총합은 반드시 1이 되어야 하는데 이를 수식으로 표현하면 다음과 같다.
∑ P(s'|s, a) = 1 (for all s ∈ S, a ∈ A, s' ∈ S) <- 확률의 기본적인 공리
확률의 총합은 모든 a에 대해 합이 1이 아닌 다음 상태 (s')에 대해 구해야 한다.
마르코프 성질에 의해 MDP의 전이 확률은 현재 상태(s)와 행동(a)에만 의존하며, 과거의 상태나 행동에는 의존하지 않고, 표준적인 MDP는 고정된 확률분포를 가정하기에 환경의 규칙(전이 확률)은 학습 도중에 변하지 않는다.

