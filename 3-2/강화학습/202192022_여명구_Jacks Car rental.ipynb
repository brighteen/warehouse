{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0300012c",
   "metadata": {},
   "source": [
    "# **강화학습 중간고사 시험 문제**\n",
    "- 코드의 단계별로 작성하고 설명 작성하세요.\n",
    "- 각각의 ipynb 파일로 제출하세요.\n",
    "- 파일명 예시: **202500000 홍길동_Jack's Car rental.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beb8d23",
   "metadata": {},
   "source": [
    "# 학번과 이름을 기입해주세요. (한글)\n",
    "\n",
    "- 학번: 202192022\n",
    "- 이름: 여명구"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c416bf7",
   "metadata": {},
   "source": [
    "## **2. Jack's Car rental**\n",
    "\n",
    "### 문제:\n",
    "- 잭(Jack)은 두 곳의 렌트카 지점(A, B)을 운영하고 있음.\n",
    "- 매일 밤 두 지점 간에 차량을 재배치하여 다음 날의 렌트 수익의 최대화하고 싶어함.\n",
    "- **수익을 최대화하기 위해서 잭(Jack)은 어떻게 해야 하는가?**\n",
    "\n",
    "### 조건\n",
    "- 수익: 차량 1대를 렌트하면 $10의 수익이 발생합니다.\n",
    "\n",
    "- 비용: 지점 간 차량 1대를 이동시키면 $2의 비용이 발생합니다.\n",
    "\n",
    "- 제약\n",
    "  1) 각 지점은 최대 20대의 차량만 보유할 수 있습니다. (초과분은 사라짐)\n",
    "  2) 밤에 이동시킬 수 있는 차량은 최대 5대입니다.\n",
    "\n",
    "\n",
    "※ 제출 내용\n",
    "  1)  최적 정책 (시각화)\n",
    "  2)  최적 가치 함수 (시각화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf439e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b80c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brigh\\Documents\\GitHub\\warehouse\\.venv\\Lib\\site-packages\\seaborn\\_statistics.py:32: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.3)\n",
      "  from scipy.stats import gaussian_kde\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== [잭의 렌터카 문제 풀이 시작] =====\n",
      "알고리즘: 정책 반복 (Policy Iteration)\n",
      "가정: γ=0.9, λ(req1,ret1)=(3,3), λ(req2,ret2)=(4,2)\n",
      "\n",
      "\n",
      "===== [정책 반복 Iteration 1] =====\n",
      "1. 정책 평가 (Policy Evaluation) 시작...\n",
      "   V 가치 최대 변화량(delta): 69.5240\n",
      "   V 가치 최대 변화량(delta): 62.3569\n",
      "   V 가치 최대 변화량(delta): 55.9161\n",
      "   V 가치 최대 변화량(delta): 49.9777\n",
      "   V 가치 최대 변화량(delta): 44.2147\n",
      "   V 가치 최대 변화량(delta): 38.5643\n",
      "   V 가치 최대 변화량(delta): 33.2485\n",
      "   V 가치 최대 변화량(delta): 28.5103\n",
      "   V 가치 최대 변화량(delta): 24.4605\n",
      "   V 가치 최대 변화량(delta): 21.0821\n",
      "   V 가치 최대 변화량(delta): 18.2882\n",
      "   V 가치 최대 변화량(delta): 15.9723\n",
      "   V 가치 최대 변화량(delta): 14.0354\n",
      "   V 가치 최대 변화량(delta): 12.3964\n",
      "   V 가치 최대 변화량(delta): 10.9925\n",
      "   V 가치 최대 변화량(delta): 9.7771\n",
      "   V 가치 최대 변화량(delta): 8.7153\n",
      "   V 가치 최대 변화량(delta): 7.7813\n",
      "   V 가치 최대 변화량(delta): 6.9552\n",
      "   V 가치 최대 변화량(delta): 6.2219\n",
      "   V 가치 최대 변화량(delta): 5.5691\n",
      "   V 가치 최대 변화량(delta): 4.9869\n",
      "   V 가치 최대 변화량(delta): 4.4668\n",
      "   V 가치 최대 변화량(delta): 4.0018\n",
      "   V 가치 최대 변화량(delta): 3.5858\n",
      "   V 가치 최대 변화량(delta): 3.2135\n",
      "   V 가치 최대 변화량(delta): 2.8800\n",
      "   V 가치 최대 변화량(delta): 2.5814\n",
      "   V 가치 최대 변화량(delta): 2.3139\n",
      "   V 가치 최대 변화량(delta): 2.0742\n",
      "   V 가치 최대 변화량(delta): 1.8594\n",
      "   V 가치 최대 변화량(delta): 1.6670\n",
      "   V 가치 최대 변화량(delta): 1.4945\n",
      "   V 가치 최대 변화량(delta): 1.3399\n",
      "   V 가치 최대 변화량(delta): 1.2013\n",
      "   V 가치 최대 변화량(delta): 1.0771\n",
      "   V 가치 최대 변화량(delta): 0.9658\n",
      "   V 가치 최대 변화량(delta): 0.8660\n",
      "   V 가치 최대 변화량(delta): 0.7765\n",
      "   V 가치 최대 변화량(delta): 0.6962\n",
      "   V 가치 최대 변화량(delta): 0.6243\n",
      "   V 가치 최대 변화량(delta): 0.5598\n",
      "   V 가치 최대 변화량(delta): 0.5020\n",
      "   V 가치 최대 변화량(delta): 0.4502\n",
      "   V 가치 최대 변화량(delta): 0.4037\n",
      "   V 가치 최대 변화량(delta): 0.3620\n",
      "   V 가치 최대 변화량(delta): 0.3247\n",
      "   V 가치 최대 변화량(delta): 0.2912\n",
      "   V 가치 최대 변화량(delta): 0.2611\n",
      "   V 가치 최대 변화량(delta): 0.2342\n",
      "   V 가치 최대 변화량(delta): 0.2100\n",
      "   V 가치 최대 변화량(delta): 0.1883\n",
      "   V 가치 최대 변화량(delta): 0.1689\n",
      "   V 가치 최대 변화량(delta): 0.1515\n",
      "   V 가치 최대 변화량(delta): 0.1358\n",
      "   V 가치 최대 변화량(delta): 0.1218\n",
      "   V 가치 최대 변화량(delta): 0.1093\n",
      "   V 가치 최대 변화량(delta): 0.0980\n",
      "   V 가치 최대 변화량(delta): 0.0879\n",
      "   V 가치 최대 변화량(delta): 0.0788\n",
      "   V 가치 최대 변화량(delta): 0.0707\n",
      "   V 가치 최대 변화량(delta): 0.0634\n",
      "   V 가치 최대 변화량(delta): 0.0569\n",
      "   V 가치 최대 변화량(delta): 0.0510\n",
      "   V 가치 최대 변화량(delta): 0.0457\n",
      "   V 가치 최대 변화량(delta): 0.0410\n",
      "   V 가치 최대 변화량(delta): 0.0368\n",
      "   V 가치 최대 변화량(delta): 0.0330\n",
      "   V 가치 최대 변화량(delta): 0.0296\n",
      "   V 가치 최대 변화량(delta): 0.0265\n",
      "   V 가치 최대 변화량(delta): 0.0238\n",
      "   V 가치 최대 변화량(delta): 0.0214\n",
      "   V 가치 최대 변화량(delta): 0.0192\n",
      "   V 가치 최대 변화량(delta): 0.0172\n",
      "   V 가치 최대 변화량(delta): 0.0154\n",
      "   V 가치 최대 변화량(delta): 0.0138\n",
      "   V 가치 최대 변화량(delta): 0.0124\n",
      "   V 가치 최대 변화량(delta): 0.0111\n",
      "   V 가치 최대 변화량(delta): 0.0100\n",
      "   V 가치 최대 변화량(delta): 0.0089\n",
      "   V 가치 최대 변화량(delta): 0.0080\n",
      "   V 가치 최대 변화량(delta): 0.0072\n",
      "   V 가치 최대 변화량(delta): 0.0065\n",
      "   V 가치 최대 변화량(delta): 0.0058\n",
      "   V 가치 최대 변화량(delta): 0.0052\n",
      "   V 가치 최대 변화량(delta): 0.0047\n",
      "   V 가치 최대 변화량(delta): 0.0042\n",
      "   V 가치 최대 변화량(delta): 0.0037\n",
      "   V 가치 최대 변화량(delta): 0.0034\n",
      "   V 가치 최대 변화량(delta): 0.0030\n",
      "   V 가치 최대 변화량(delta): 0.0027\n",
      "   V 가치 최대 변화량(delta): 0.0024\n",
      "   V 가치 최대 변화량(delta): 0.0022\n",
      "   V 가치 최대 변화량(delta): 0.0019\n",
      "   V 가치 최대 변화량(delta): 0.0017\n",
      "   V 가치 최대 변화량(delta): 0.0016\n",
      "   V 가치 최대 변화량(delta): 0.0014\n",
      "   V 가치 최대 변화량(delta): 0.0013\n",
      "   V 가치 최대 변화량(delta): 0.0011\n",
      "   V 가치 최대 변화량(delta): 0.0010\n",
      "   V 가치 최대 변화량(delta): 0.0009\n",
      "   V 가치 최대 변화량(delta): 0.0008\n",
      "   V 가치 최대 변화량(delta): 0.0007\n",
      "   V 가치 최대 변화량(delta): 0.0007\n",
      "   V 가치 최대 변화량(delta): 0.0006\n",
      "   V 가치 최대 변화량(delta): 0.0005\n",
      "   V 가치 최대 변화량(delta): 0.0005\n",
      "   V 가치 최대 변화량(delta): 0.0004\n",
      "   V 가치 최대 변화량(delta): 0.0004\n",
      "   V 가치 최대 변화량(delta): 0.0003\n",
      "   V 가치 최대 변화량(delta): 0.0003\n",
      "   V 가치 최대 변화량(delta): 0.0003\n",
      "   V 가치 최대 변화량(delta): 0.0002\n",
      "   V 가치 최대 변화량(delta): 0.0002\n",
      "   V 가치 최대 변화량(delta): 0.0002\n",
      "   V 가치 최대 변화량(delta): 0.0002\n",
      "   V 가치 최대 변화량(delta): 0.0002\n",
      "   V 가치 최대 변화량(delta): 0.0001\n",
      "   V 가치 최대 변화량(delta): 0.0001\n",
      "   V 가치 최대 변화량(delta): 0.0001\n",
      "   V 가치 최대 변화량(delta): 0.0001\n",
      "   V 가치 최대 변화량(delta): 0.0001\n",
      "   정책 평가 완료. V 수렴.\n",
      "2. 정책 개선 (Policy Improvement) 시작...\n",
      "정책 개선 완료. 정책이 변경됨(Unstable).\n",
      "\n",
      "===== [정책 반복 Iteration 2] =====\n",
      "1. 정책 평가 (Policy Evaluation) 시작...\n",
      "   V 가치 최대 변화량(delta): 35.3897\n",
      "   V 가치 최대 변화량(delta): 21.4892\n",
      "   V 가치 최대 변화량(delta): 7.6211\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 250\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m가정: γ=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGAMMA\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, λ(req1,ret1)=(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLAMBDA_REQ_1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLAMBDA_RET_1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m), λ(req2,ret2)=(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLAMBDA_REQ_2\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLAMBDA_RET_2\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# 1. 문제 풀이\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m optimal_policy, optimal_value = \u001b[43mpolicy_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;66;03m# 2. 결과 출력 (중요 요소 확인)\u001b[39;00m\n\u001b[32m    253\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- [제출 1] 최적 정책 (π*) ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 140\u001b[39m, in \u001b[36mpolicy_iteration\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    137\u001b[39m         action = Policy[state] \n\u001b[32m    139\u001b[39m         \u001b[38;5;66;03m# V(k+1)[s] <- E[R + γV(k)[s']]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m         V[state] = \u001b[43mcalculate_expected_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV_old\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m         delta = \u001b[38;5;28mmax\u001b[39m(delta, \u001b[38;5;28mabs\u001b[39m(V[state] - V_old[state]))\n\u001b[32m    144\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   V 가치 최대 변화량(delta): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelta\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mcalculate_expected_return\u001b[39m\u001b[34m(state, action, V)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ret1 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_POISSON_K):\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ret2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_POISSON_K):\n\u001b[32m     77\u001b[39m \n\u001b[32m     78\u001b[39m         \u001b[38;5;66;03m# 4. 이 사건이 발생할 확률 (P)\u001b[39;00m\n\u001b[32m     79\u001b[39m         \u001b[38;5;66;03m# 모든 사건은 독립적이므로 확률을 곱함 [cite: 204]\u001b[39;00m\n\u001b[32m     80\u001b[39m         prob = (\n\u001b[32m     81\u001b[39m             get_poisson_prob(req1, LAMBDA_REQ_1) *\n\u001b[32m     82\u001b[39m             get_poisson_prob(req2, LAMBDA_REQ_2) *\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m             \u001b[43mget_poisson_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLAMBDA_RET_1\u001b[49m\u001b[43m)\u001b[49m *\n\u001b[32m     84\u001b[39m             get_poisson_prob(ret2, LAMBDA_RET_2)\n\u001b[32m     85\u001b[39m         )\n\u001b[32m     87\u001b[39m         \u001b[38;5;66;03m# 5. 실제 렌트된 차량 수 (수요가 재고보다 많을 수 없음)\u001b[39;00m\n\u001b[32m     88\u001b[39m         rented1 = \u001b[38;5;28mmin\u001b[39m(n1_start, req1)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mget_poisson_prob\u001b[39m\u001b[34m(k, lam)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# 포아송 확률값 캐시 (매번 계산하는 것을 방지)\u001b[39;00m\n\u001b[32m     43\u001b[39m poisson_cache = \u001b[38;5;28mdict\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_poisson_prob\u001b[39m(k, lam):\n\u001b[32m     46\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[33;03m    k와 lambda에 대한 포아송 확률(PMF)을 계산합니다.\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[33;03m    (계산된 값을 캐시에 저장하여 중복 계산 방지)\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (k, lam) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m poisson_cache:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "이 코드는 '잭의 렌터카(Jack's Car Rental)' 문제를 해결하기 위해\n",
    "'정책 반복(Policy Iteration)' 알고리즘을 구현합니다.\n",
    "\n",
    "이 문제는 PDF 자료에서 제시된 MDP(S, A, P, R) 정의를 따릅니다.\n",
    "- S (상태): (A지점 차량 수, B지점 차량 수) [cite: 189]\n",
    "- A (행동): A->B로 이동하는 차량 수 (-5 ~ +5) [cite: 192]\n",
    "- P (전이): 수요/반납이 포아송 분포를 따름 [cite: 152, 202, 203]\n",
    "- R (보상): 대여료($10) - 이동비용($2) [cite: 148]\n",
    "\n",
    "알고리즘은 [정책 평가]와 [정책 개선]을 수렴할 때까지 반복합니다[cite: 221].\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # 시각화를 위한 Seaborn 사용\n",
    "from scipy.stats import poisson\n",
    "\n",
    "# --- 1. 환경 및 문제 조건 설정 ---\n",
    "\n",
    "# 제약 조건\n",
    "MAX_CARS = 20           # 각 지점 최대 차량 수 [cite: 143]\n",
    "MAX_MOVE = 5            # 밤 사이 최대 이동 가능 차량 수 [cite: 145]\n",
    "STATE_SPACE = MAX_CARS + 1 # (0 ~ 20, 총 21개 상태)\n",
    "\n",
    "# 보상 및 비용\n",
    "RENTAL_PROFIT = 10      # 대여 수익 [cite: 148]\n",
    "MOVE_COST = 2           # 이동 비용\n",
    "\n",
    "# MDP 파라미터\n",
    "GAMMA = 0.9             # 할인율 (PDF 예시) [cite: 217]\n",
    "\n",
    "# PDF에서 가정한 포아송 분포 파라미터 [cite: 153, 154]\n",
    "LAMBDA_REQ_1 = 3  # 1번 지점(A) 평균 요청\n",
    "LAMBDA_RET_1 = 3  # 1번 지점(A) 평균 반납\n",
    "LAMBDA_REQ_2 = 4  # 2번 지점(B) 평균 요청\n",
    "LAMBDA_RET_2 = 2  # 2번 지점(B) 평균 반납\n",
    "\n",
    "# 계산 최적화: 포아송 분포는 꼬리가 길기 때문에\n",
    "# 확률이 매우 낮은 (e.g., 11대 이상) 경우는 계산에서 제외 [cite: 211]\n",
    "MAX_POISSON_K = 11      \n",
    "# 포아송 확률값 캐시 (매번 계산하는 것을 방지)\n",
    "poisson_cache = dict()\n",
    "\n",
    "def get_poisson_prob(k, lam):\n",
    "    \"\"\"\n",
    "    k와 lambda에 대한 포아송 확률(PMF)을 계산합니다.\n",
    "    (계산된 값을 캐시에 저장하여 중복 계산 방지)\n",
    "    \"\"\"\n",
    "    if (k, lam) not in poisson_cache:\n",
    "        poisson_cache[(k, lam)] = poisson.pmf(k, lam)\n",
    "    return poisson_cache[(k, lam)]\n",
    "\n",
    "# --- 2. 핵심 함수: 기대 보상 계산 (4중 루프) ---\n",
    "\n",
    "def calculate_expected_return(state, action, V):\n",
    "    \"\"\"\n",
    "    현재 상태(s), 행동(a), 현재 가치함수(V)를 바탕으로\n",
    "    벨만 방정식의 우변(기대 보상 + 기대 가치)을 계산합니다.\n",
    "    이것이 이 문제의 가장 복잡한 '전이(P)'와 '보상(R)' 부분입니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 즉각적인 보상(비용)\n",
    "    # 행동(action)은 1->2로 옮기는 차의 수.\n",
    "    expected_return = -MOVE_COST * abs(action)\n",
    "    \n",
    "    # 2. 행동(a) 적용 후 아침 차량 대수\n",
    "    # (제약 조건 1: 최대 20대)\n",
    "    n1_start = min(state[0] - action, MAX_CARS)\n",
    "    n2_start = min(state[1] + action, MAX_CARS)\n",
    "\n",
    "    # 3. 모든 확률적 사건(수요/반납)에 대한 기댓값 계산 (4중 루프)\n",
    "    for req1 in range(MAX_POISSON_K):\n",
    "        for req2 in range(MAX_POISSON_K):\n",
    "            for ret1 in range(MAX_POISSON_K):\n",
    "                for ret2 in range(MAX_POISSON_K):\n",
    "                    \n",
    "                    # 4. 이 사건이 발생할 확률 (P)\n",
    "                    # 모든 사건은 독립적이므로 확률을 곱함 [cite: 204]\n",
    "                    prob = (\n",
    "                        get_poisson_prob(req1, LAMBDA_REQ_1) *\n",
    "                        get_poisson_prob(req2, LAMBDA_REQ_2) *\n",
    "                        get_poisson_prob(ret1, LAMBDA_RET_1) *\n",
    "                        get_poisson_prob(ret2, LAMBDA_RET_2)\n",
    "                    )\n",
    "                    \n",
    "                    # 5. 실제 렌트된 차량 수 (수요가 재고보다 많을 수 없음)\n",
    "                    rented1 = min(n1_start, req1)\n",
    "                    rented2 = min(n2_start, req2)\n",
    "                    \n",
    "                    # 6. 확률적 보상 (R)\n",
    "                    profit = (rented1 + rented2) * RENTAL_PROFIT\n",
    "                    \n",
    "                    # 7. 하루 종료 시 차량 수 (다음 상태 s')\n",
    "                    # (제약 조건 1: 최대 20대) [cite: 207]\n",
    "                    n1_end = min(n1_start - rented1 + ret1, MAX_CARS)\n",
    "                    n2_end = min(n2_start - rented2 + ret2, MAX_CARS)\n",
    "                    \n",
    "                    # 8. E[R + γV(s')] 누적\n",
    "                    # (V[n1_end, n2_end]는 다음 상태의 가치)\n",
    "                    expected_return += prob * (profit + GAMMA * V[n1_end, n2_end])\n",
    "\n",
    "    return expected_return\n",
    "\n",
    "# --- 3. 알고리즘: 정책 반복 (Policy Iteration) ---\n",
    "\n",
    "def policy_iteration():\n",
    "    \"\"\"\n",
    "    정책 반복(Policy Iteration)을 수행하여\n",
    "    최적 정책(Policy)과 최적 가치(V)를 찾습니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 초기화\n",
    "    # V: 21x21 크기, 0으로 초기화\n",
    "    V = np.zeros((STATE_SPACE, STATE_SPACE)) \n",
    "    # Policy: 21x21 크기, 0으로 초기화 (행동: \"이동 안 함\")\n",
    "    Policy = np.zeros((STATE_SPACE, STATE_SPACE), dtype=int) \n",
    "    \n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        print(f\"\\n===== [정책 반복 Iteration {iteration}] =====\")\n",
    "        \n",
    "        # --- [STEP 1: 정책 평가 (Policy Evaluation)] ---\n",
    "        # 현재 정책(Policy)이 얼마나 좋은지, V가 수렴할 때까지 계산 [cite: 222]\n",
    "        print(\"1. 정책 평가 (Policy Evaluation) 시작...\")\n",
    "        while True:\n",
    "            delta = 0\n",
    "            # V(k+1) 계산을 위해 V(k)를 복사\n",
    "            V_old = V.copy() \n",
    "            \n",
    "            # 441개 모든 상태(s)에 대해\n",
    "            for n1 in range(STATE_SPACE):\n",
    "                for n2 in range(STATE_SPACE):\n",
    "                    state = (n1, n2)\n",
    "                    # 현재 정책(Policy)이 정해준 행동(a)\n",
    "                    action = Policy[state] \n",
    "                    \n",
    "                    # V(k+1)[s] <- E[R + γV(k)[s']]\n",
    "                    V[state] = calculate_expected_return(state, action, V_old)\n",
    "                    \n",
    "                    delta = max(delta, abs(V[state] - V_old[state]))\n",
    "            \n",
    "            print(f\"   V 가치 최대 변화량(delta): {delta:.4f}\")\n",
    "            # 가치 함수가 수렴하면 평가 종료\n",
    "            if delta < 1e-4: # (수렴 기준값)\n",
    "                print(\"   정책 평가 완료. V 수렴.\")\n",
    "                break\n",
    "\n",
    "        # --- [STEP 2: 정책 개선 (Policy Improvement)] ---\n",
    "        # 계산된 V를 바탕으로 더 나은 행동이 있는지 탐색 [cite: 223]\n",
    "        print(\"2. 정책 개선 (Policy Improvement) 시작...\")\n",
    "        policy_stable = True\n",
    "        \n",
    "        for n1 in range(STATE_SPACE):\n",
    "            for n2 in range(STATE_SPACE):\n",
    "                state = (n1, n2)\n",
    "                old_action = Policy[state]\n",
    "                \n",
    "                action_returns = []\n",
    "                \n",
    "                # 제약 2)를 고려한 가능한 모든 행동 (-5 ~ +5)\n",
    "                # (단, 현재 재고를 초과할 수 없음)\n",
    "                valid_actions = range(\n",
    "                    max(-MAX_MOVE, -n2),               # B에서 5대 or n2대 이상 못 뺌\n",
    "                    min(MAX_MOVE, n1) + 1              # A에서 5대 or n1대 이상 못 뺌\n",
    "                )\n",
    "                \n",
    "                # 모든 가능한 행동 'a'에 대해\n",
    "                for action in valid_actions:\n",
    "                    # Q(s, a) = E[R + γV(s')] 계산\n",
    "                    q_value = calculate_expected_return(state, action, V)\n",
    "                    action_returns.append(q_value)\n",
    "                \n",
    "                # 가장 좋은(Greedy) 행동을 새 정책으로 선택\n",
    "                new_action = valid_actions[np.argmax(action_returns)]\n",
    "                Policy[state] = new_action\n",
    "                \n",
    "                # 정책이 하나라도 바뀌었는지 확인\n",
    "                if old_action != new_action:\n",
    "                    policy_stable = False\n",
    "        \n",
    "        if policy_stable:\n",
    "            print(f\"정책 개선 완료. 정책이 안정됨(Stable).\")\n",
    "            print(f\"===== [최적 정책 발견! Iteration {iteration}에서 종료] =====\")\n",
    "            break # 메인 루프 종료\n",
    "        else:\n",
    "            print(f\"정책 개선 완료. 정책이 변경됨(Unstable).\")\n",
    "\n",
    "    return Policy, V\n",
    "\n",
    "# --- 4. 시각화 함수 ---\n",
    "\n",
    "def plot_results(Policy, V):\n",
    "    \"\"\"\n",
    "    최종 최적 정책(π*)과 최적 가치(V*)를 시각화합니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 플롯 1: 최적 가치 함수 (V*) - 3D 표면 플롯\n",
    "    # (PDF 12페이지의 V4 그래프와 유사 [cite: 179])\n",
    "    fig = plt.figure(figsize=(12, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    X = np.arange(0, STATE_SPACE) # Loc 2 (0~20)\n",
    "    Y = np.arange(0, STATE_SPACE) # Loc 1 (0~20)\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    \n",
    "    # (V 배열의 (n1, n2) 축이 (Y, X)와 일치함)\n",
    "    ax.plot_surface(X, Y, V, cmap='viridis')\n",
    "    ax.set_title('Optimal Value Function (V*)')\n",
    "    ax.set_xlabel('# Cars at Location 2')\n",
    "    ax.set_ylabel('# Cars at Location 1')\n",
    "    ax.set_zlabel('Expected Return ($)')\n",
    "    plt.show()\n",
    "\n",
    "    # 플롯 2: 최적 정책 (π*) - 2D 히트맵\n",
    "    # (PDF 12페이지의 π4 그래프와 유사 [cite: 169])\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # (정책 배열을 뒤집어야 (0,0)이 좌하단에 옴)\n",
    "    Policy_flipped = np.flipud(Policy) \n",
    "    \n",
    "    ax = sns.heatmap(\n",
    "        Policy_flipped, \n",
    "        cmap='coolwarm',  # (양수: A->B(파랑), 음수: B->A(빨강))\n",
    "        center=0,         # 0을 흰색으로\n",
    "        annot=True,       # 숫자 표시\n",
    "        fmt='d',          # 정수로\n",
    "        linewidths=.5\n",
    "    )\n",
    "    ax.set_title('Optimal Policy (π*) [Cars moved from Loc 1 to Loc 2]')\n",
    "    ax.set_xlabel('# Cars at Location 2')\n",
    "    ax.set_ylabel('# Cars at Location 1')\n",
    "    # Y축 레이블을 (0, 5, ..., 20)으로 수정 (뒤집었기 때문)\n",
    "    ax.set_yticklabels(\n",
    "        np.arange(MAX_CARS, -1, -5), \n",
    "        rotation=0, \n",
    "        va='center'\n",
    "    )\n",
    "    ax.set_xticklabels(np.arange(0, STATE_SPACE, 5))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- 5. 메인 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"===== [잭의 렌터카 문제 풀이 시작] =====\")\n",
    "    print(\"알고리즘: 정책 반복 (Policy Iteration)\")\n",
    "    print(f\"가정: γ={GAMMA}, λ(req1,ret1)=({LAMBDA_REQ_1},{LAMBDA_RET_1}), λ(req2,ret2)=({LAMBDA_REQ_2},{LAMBDA_RET_2})\\n\")\n",
    "    \n",
    "    # 1. 문제 풀이\n",
    "    optimal_policy, optimal_value = policy_iteration()\n",
    "    \n",
    "    # 2. 결과 출력 (중요 요소 확인)\n",
    "    print(\"\\n\\n--- [제출 1] 최적 정책 (π*) ---\")\n",
    "    print(\"(각 칸은 (A지점 차량, B지점 차량) 상태에서 A->B로 이동시킬 차량 수)\")\n",
    "    print(optimal_policy)\n",
    "    \n",
    "    print(\"\\n\\n--- [제출 2] 최적 가치 함수 (V*) ---\")\n",
    "    print(\"(각 칸은 해당 상태의 총 기대 수익)\")\n",
    "    print(optimal_value.round(1))\n",
    "    \n",
    "    # 3. 결과 시각화\n",
    "    print(\"\\n\\n--- [시각화] 결과 플롯 생성 중 ---\")\n",
    "    plot_results(optimal_policy, optimal_value)\n",
    "    \n",
    "    print(\"===== [문제 풀이 완료] =====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb97d10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
