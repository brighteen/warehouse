# 파라미터 수 계산
패딩, 스트라이드, 풀링은 하이퍼파라미터임.
파라미터 수는 필터(커널, 가중치)에 의해 결정됨.
일반적인 합성곱에서 필터의 개수(k)에 따라 output의 채널(k)이 결정됨.
필터의 채널(c)는 입력층의 채널(c)과 동일해야 함.



## batch normalization이 있을때 파라미터 수 계산
파라미터 4개 중 2개는 이동평균과 분산으로 학습되지 않는 파라미터임.
학습되는 파라미터는 스케일(gamma), 시프트(beta) 2개임.

---

bottleneck이란 고차원 데이터를 저차원으로 압축시켜도 모델이 데이터를 표현할 수 있다면 이는 매니폴드관점에서 데이터가 보다 저차원에 존재한다고 볼 수도 있음.

배치사이즈
mnist 6만개 데이터를 학습할 때 
배치 사이즈가 1이면 모델이 한번 학습할 때 1개씩 데이터를 가져와서 학습함. 이때 한번의 에포크에서 6만번 가중치를 업데이트함. 모델이 각 하나씩 보기 때문에 각 업데이트는 샘플에 따라 편향되있음. 학습속도가 느리지만 메모리 사용량이 적고 일반화 성능이 좋음.
배치 사이즈가 10이면 모델이 한번 학습할 때 10개씩 데이터를 가져와서 학습함.
배치 사이즈가 6만이면 모델이 한번 학습할 때 6만개 데이터를 한번에 봄. 이때 한번의 에포크에서 1번만 가중치를 업데이트함. 학습속도가 빠르지만 메모리 사용량이 많고 일반화 성능이 떨어질 수 있음(과적합).

FLOPs: 초당 부동소수점 연산 횟수
EfficientNet에서 Compound Scaleing으로 모델의 표현력(깊이, 너비, 해상도)을 한번에 조정하는 법을 제안.

SENet
Squeeze and Excitation: 채널별 중요도산출 후, 특징맵 채널별로 곱하는 과정

MobileNet
Depthwise convolution: 각 채널별로 필터를 적용.

Pointwise convolution: 1x1 합성곱으로 채널간 선형조합 수행.

Residual block vs MobileNet block(Invered Residual block)
Residual block: 입력 -> Conv(3x3) -> Conv(3x3) -> 출력 + 입력 (bottleneck, 정보 병목)
Invered Residual block: 입력 -> Conv(1x1, 확장) -> MBConv(3x3) -> Conv(1x1, 축소) -> 출력 + 입력 (expansion, 정보 팽창)