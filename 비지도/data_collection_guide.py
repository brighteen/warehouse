# Image-to-Image í›ˆë ¨ìš© ë°ì´í„° ìˆ˜ì§‘ ê°€ì´ë“œ

"""
í˜„ì¬ ìƒí™©: ë°˜ ê³ í ì‘í’ˆ 250+ê°œ ë³´ìœ 
ëª©í‘œ: Style Transfer ëª¨ë¸ í›ˆë ¨

í•„ìš”í•œ ì¶”ê°€ ë°ì´í„°:
1. ì†ŒìŠ¤ ì´ë¯¸ì§€ (ì¼ë°˜ ì‚¬ì§„ë“¤)
2. ì¶©ë¶„í•œ ì–‘ê³¼ ë‹¤ì–‘ì„±
"""

import os
import requests
import zipfile
from PIL import Image
import matplotlib.pyplot as plt

# ================================================================
# 1. ë¬´ë£Œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì˜µì…˜ë“¤
# ================================================================

def download_coco_subset():
    """COCO ë°ì´í„°ì…‹ ì¼ë¶€ ë‹¤ìš´ë¡œë“œ (ì¼ë°˜ ì‚¬ì§„ìš©)"""
    print("ğŸ”½ COCO ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
    
    # COCO 2017 validation set (ì‘ì€ í¬ê¸°)
    url = "http://images.cocodataset.org/zips/val2017.zip"
    
    # ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
    download_dir = r'C:\Users\brigh\Documents\GitHub\warehouse\ë¹„ì§€ë„\source_images'
    os.makedirs(download_dir, exist_ok=True)
    
    print(f"ë‹¤ìš´ë¡œë“œ ìœ„ì¹˜: {download_dir}")
    print("âš ï¸  ì£¼ì˜: 1GB ì •ë„ ìš©ëŸ‰ì´ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    """
    # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ì½”ë“œ (í•„ìš”ì‹œ í™œì„±í™”)
    response = requests.get(url, stream=True)
    zip_path = os.path.join(download_dir, "val2017.zip")
    
    with open(zip_path, 'wb') as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    
    # ì••ì¶• í•´ì œ
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(download_dir)
    
    print("âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
    """

def alternative_datasets():
    """ëŒ€ì•ˆ ë°ì´í„°ì…‹ ì˜µì…˜ë“¤"""
    print("ğŸ¯ ì¶”ì²œ ë°ì´í„°ì…‹ ì˜µì…˜ë“¤:")
    print()
    
    print("1ï¸âƒ£ **Places365** (í’ê²½ ì‚¬ì§„)")
    print("   - 365ê°œ ì¥ì†Œ ì¹´í…Œê³ ë¦¬")
    print("   - ê³ í™”ì§ˆ í’ê²½/ê±´ë¬¼ ì‚¬ì§„")
    print("   - ë‹¤ìš´ë¡œë“œ: http://places2.csail.mit.edu/download.html")
    print()
    
    print("2ï¸âƒ£ **ImageNet** (ì¼ë°˜ ê°ì²´)")
    print("   - ë‹¤ì–‘í•œ ê°ì²´ í´ë˜ìŠ¤")
    print("   - ê³ í’ˆì§ˆ ì´ë¯¸ì§€")
    print("   - ë‹¤ìš´ë¡œë“œ: https://www.image-net.org/")
    print()
    
    print("3ï¸âƒ£ **Flickr í¬ë¦¬ì—ì´í‹°ë¸Œ ì»¤ë¨¼ì¦ˆ**")
    print("   - ì‹¤ì œ ì‚¬ìš©ì ì´¬ì˜ ì‚¬ì§„")
    print("   - ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ë„ì™€ ìƒ‰ê°")
    print("   - API ë˜ëŠ” ìˆ˜ë™ ìˆ˜ì§‘")
    print()
    
    print("4ï¸âƒ£ **Unsplash API** (ê³ í’ˆì§ˆ)")
    print("   - ì „ë¬¸ê°€ê¸‰ ì‚¬ì§„")
    print("   - ë‹¤ì–‘í•œ ì£¼ì œ")
    print("   - API í‚¤ í•„ìš”")

def create_data_structure():
    """í›ˆë ¨ìš© ë°ì´í„° í´ë” êµ¬ì¡° ìƒì„±"""
    base_dir = r'C:\Users\brigh\Documents\GitHub\warehouse\ë¹„ì§€ë„'
    
    # í´ë” êµ¬ì¡° ìƒì„±
    folders = [
        'training_data/source_images',      # ì¼ë°˜ ì‚¬ì§„ (ì†ŒìŠ¤)
        'training_data/target_images',      # ë°˜ ê³ í ì‘í’ˆ (íƒ€ê²Ÿ)
        'training_data/validation',         # ê²€ì¦ìš©
        'trained_models',                   # í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥
        'results'                          # ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥
    ]
    
    for folder in folders:
        folder_path = os.path.join(base_dir, folder)
        os.makedirs(folder_path, exist_ok=True)
        print(f"ğŸ“ ìƒì„±ë¨: {folder_path}")
    
    # ê¸°ì¡´ ë°˜ ê³ í ì‘í’ˆë“¤ì„ target_imagesë¡œ ë³µì‚¬í•˜ëŠ” ì½”ë“œ ì œì•ˆ
    artwork_source = os.path.join(base_dir, 'artwork_data')
    artwork_target = os.path.join(base_dir, 'training_data', 'target_images')
    
    print(f"\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"1. {artwork_source}ì˜ ì´ë¯¸ì§€ë“¤ì„ {artwork_target}ë¡œ ë³µì‚¬")
    print(f"2. ì¼ë°˜ ì‚¬ì§„ë“¤ì„ training_data/source_imagesì— ì¶”ê°€")
    print(f"3. ì´ë¯¸ì§€ í¬ê¸° ë° í’ˆì§ˆ í™•ì¸")

# ================================================================
# 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ê²€ì¦
# ================================================================

def validate_dataset(source_dir, target_dir):
    """ë°ì´í„°ì…‹ ìœ íš¨ì„± ê²€ì‚¬"""
    print("ğŸ” ë°ì´í„°ì…‹ ê²€ì¦ ì¤‘...")
    
    # ì´ë¯¸ì§€ ê°œìˆ˜ í™•ì¸
    source_images = [f for f in os.listdir(source_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    target_images = [f for f in os.listdir(target_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    
    print(f"ì†ŒìŠ¤ ì´ë¯¸ì§€ ìˆ˜: {len(source_images)}")
    print(f"íƒ€ê²Ÿ ì´ë¯¸ì§€ ìˆ˜: {len(target_images)}")
    
    # ê¶Œì¥ ë¹„ìœ¨ í™•ì¸
    if len(source_images) < len(target_images):
        print("âš ï¸  ê²½ê³ : ì†ŒìŠ¤ ì´ë¯¸ì§€ê°€ íƒ€ê²Ÿë³´ë‹¤ ì ìŠµë‹ˆë‹¤.")
        print("   ê¶Œì¥: ì†ŒìŠ¤ >= íƒ€ê²Ÿ (ìµœì†Œ 1:1, ì´ìƒì ìœ¼ë¡œëŠ” 3:1)")
    
    # ì´ë¯¸ì§€ í¬ê¸° ë¶„ì„
    print("\nğŸ“ ì´ë¯¸ì§€ í¬ê¸° ë¶„ì„:")
    sample_sizes = []
    
    for img_file in source_images[:10]:  # ìƒ˜í”Œ 10ê°œë§Œ í™•ì¸
        try:
            img_path = os.path.join(source_dir, img_file)
            img = Image.open(img_path)
            sample_sizes.append(img.size)
        except Exception as e:
            print(f"ì˜¤ë¥˜ íŒŒì¼: {img_file} - {e}")
    
    if sample_sizes:
        avg_width = sum(size[0] for size in sample_sizes) / len(sample_sizes)
        avg_height = sum(size[1] for size in sample_sizes) / len(sample_sizes)
        print(f"í‰ê·  í¬ê¸°: {avg_width:.0f} x {avg_height:.0f}")
        
        if avg_width < 256 or avg_height < 256:
            print("âš ï¸  ê²½ê³ : ì´ë¯¸ì§€ í•´ìƒë„ê°€ ë‚®ìŠµë‹ˆë‹¤. 256x256 ì´ìƒ ê¶Œì¥")

def preprocess_images(input_dir, output_dir, target_size=(256, 256)):
    """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (í¬ê¸° ì¡°ì •, í¬ë§· í†µì¼)"""
    print(f"ğŸ”„ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹œì‘: {input_dir} â†’ {output_dir}")
    
    os.makedirs(output_dir, exist_ok=True)
    
    image_files = [f for f in os.listdir(input_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    
    for i, img_file in enumerate(image_files):
        try:
            input_path = os.path.join(input_dir, img_file)
            output_path = os.path.join(output_dir, f"processed_{i:04d}.jpg")
            
            # ì´ë¯¸ì§€ ë¡œë“œ ë° ë¦¬ì‚¬ì´ì¦ˆ
            img = Image.open(input_path).convert('RGB')
            img_resized = img.resize(target_size, Image.Resampling.LANCZOS)
            
            # ì €ì¥
            img_resized.save(output_path, 'JPEG', quality=95)
            
            if i % 100 == 0:
                print(f"ì²˜ë¦¬ ì™„ë£Œ: {i}/{len(image_files)}")
                
        except Exception as e:
            print(f"ì˜¤ë¥˜ íŒŒì¼: {img_file} - {e}")
    
    print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(image_files)}ê°œ ì´ë¯¸ì§€")

# ================================================================
# 3. ì‹¤ì œ ì‹¤í–‰ ì˜ˆì œ
# ================================================================

def setup_data_for_training():
    """í›ˆë ¨ìš© ë°ì´í„° ì„¤ì • ì™„ì „ ê°€ì´ë“œ"""
    print("="*60)
    print("ğŸ¨ Image-to-Image í›ˆë ¨ ë°ì´í„° ì„¤ì • ê°€ì´ë“œ")
    print("="*60)
    
    # 1. í´ë” êµ¬ì¡° ìƒì„±
    print("\n1ï¸âƒ£ í´ë” êµ¬ì¡° ìƒì„±...")
    create_data_structure()
    
    # 2. ë°ì´í„°ì…‹ ì˜µì…˜ ì•ˆë‚´
    print("\n2ï¸âƒ£ ë°ì´í„°ì…‹ ìˆ˜ì§‘ ì˜µì…˜:")
    alternative_datasets()
    
    # 3. ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
    print("\n3ï¸âƒ£ ë‹¤ìŒ ë‹¨ê³„:")
    print("   a) ì¼ë°˜ ì‚¬ì§„ ë°ì´í„° ìˆ˜ì§‘ (ìœ„ ì˜µì…˜ ì¤‘ ì„ íƒ)")
    print("   b) ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰")
    print("   c) í›ˆë ¨ ì‹œì‘")
    
    print("\n4ï¸âƒ£ ì˜ˆìƒ í›ˆë ¨ ì‹œê°„:")
    print("   - ë°ì´í„° 1,000ì¥: 2-4ì‹œê°„ (GPU ê¸°ì¤€)")
    print("   - ë°ì´í„° 10,000ì¥: 1-2ì¼")
    print("   - ì—í¬í¬ 100íšŒ ê¸°ì¤€")

if __name__ == "__main__":
    setup_data_for_training()
