# Image-to-Image ëª¨ë¸ íŒŒì¸íŠœë‹ ê°€ì´ë“œ
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import os
import matplotlib.pyplot as plt

# ================================================================
# 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
# ================================================================

"""
í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜:
pip install torch torchvision
pip install pillow
pip install matplotlib
pip install opencv-python

ì„ íƒì  (ê³ ê¸‰ ê¸°ëŠ¥):
pip install pytorch-fid  # FID ìŠ¤ì½”ì–´ ê³„ì‚°
pip install lpips        # ì§€ê°ì  ì†ì‹¤ ê³„ì‚°
"""

# ================================================================
# 2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
# ================================================================

class ImageToImageDataset(Dataset):
    """Image-to-Image ë³€í™˜ì„ ìœ„í•œ ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    
    def __init__(self, source_dir, target_dir, transform=None, paired=False):
        """
        Args:
            source_dir: ì…ë ¥ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ (ì¼ë°˜ ì‚¬ì§„)
            target_dir: íƒ€ê²Ÿ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ (ë°˜ ê³ í ì‘í’ˆ)
            transform: ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            paired: Trueë©´ ê°™ì€ íŒŒì¼ëª…ë¼ë¦¬ ë§¤ì¹­, Falseë©´ ëœë¤ ë§¤ì¹­
        """
        self.source_dir = source_dir
        self.target_dir = target_dir
        self.transform = transform
        self.paired = paired
        
        # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
        self.source_images = [f for f in os.listdir(source_dir) 
                             if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        self.target_images = [f for f in os.listdir(target_dir) 
                             if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        
        # pairedê°€ Falseë©´ ë” ë§ì€ ì¡°í•© ìƒì„± ê°€ëŠ¥
        self.length = max(len(self.source_images), len(self.target_images))
    
    def __len__(self):
        return self.length
    
    def __getitem__(self, idx):
        # ì†ŒìŠ¤ ì´ë¯¸ì§€ ì„ íƒ
        source_idx = idx % len(self.source_images)
        source_path = os.path.join(self.source_dir, self.source_images[source_idx])
        source_img = Image.open(source_path).convert('RGB')
        
        # íƒ€ê²Ÿ ì´ë¯¸ì§€ ì„ íƒ
        if self.paired:
            # ê°™ì€ ì¸ë±ìŠ¤ ì‚¬ìš© (paired ë°ì´í„°)
            target_idx = source_idx % len(self.target_images)
        else:
            # ëœë¤ ì„ íƒ (unpaired ë°ì´í„°)
            target_idx = torch.randint(0, len(self.target_images), (1,)).item()
        
        target_path = os.path.join(self.target_dir, self.target_images[target_idx])
        target_img = Image.open(target_path).convert('RGB')
        
        # ì „ì²˜ë¦¬ ì ìš©
        if self.transform:
            source_img = self.transform(source_img)
            target_img = self.transform(target_img)
        
        return source_img, target_img

# ================================================================
# 3. ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜ (U-Net ê¸°ë°˜)
# ================================================================

class UNetBlock(nn.Module):
    """U-Netì˜ ê¸°ë³¸ ë¸”ë¡"""
    def __init__(self, in_channels, out_channels, down=True, use_dropout=False):
        super().__init__()
        if down:
            self.conv = nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False)
        else:
            self.conv = nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False)
        
        self.norm = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU() if down else nn.ReLU(inplace=True)
        self.dropout = nn.Dropout(0.5) if use_dropout else None
    
    def forward(self, x):
        x = self.conv(x)
        x = self.norm(x)
        x = self.relu(x)
        if self.dropout:
            x = self.dropout(x)
        return x

class Generator(nn.Module):
    """ì´ë¯¸ì§€ ìƒì„±ê¸° (U-Net êµ¬ì¡°)"""
    def __init__(self, in_channels=3, out_channels=3):
        super().__init__()
        
        # Encoder (ë‹¤ìš´ìƒ˜í”Œë§)
        self.down1 = nn.Conv2d(in_channels, 64, 4, 2, 1)  # 128x128
        self.down2 = UNetBlock(64, 128, down=True)         # 64x64
        self.down3 = UNetBlock(128, 256, down=True)        # 32x32
        self.down4 = UNetBlock(256, 512, down=True)        # 16x16
        self.down5 = UNetBlock(512, 512, down=True)        # 8x8
        self.down6 = UNetBlock(512, 512, down=True)        # 4x4
        
        # Bottleneck
        self.bottleneck = UNetBlock(512, 512, down=True)   # 2x2
        
        # Decoder (ì—…ìƒ˜í”Œë§)
        self.up1 = UNetBlock(512, 512, down=False, use_dropout=True)     # 4x4
        self.up2 = UNetBlock(1024, 512, down=False, use_dropout=True)    # 8x8
        self.up3 = UNetBlock(1024, 512, down=False, use_dropout=True)    # 16x16
        self.up4 = UNetBlock(1024, 256, down=False)                      # 32x32
        self.up5 = UNetBlock(512, 128, down=False)                       # 64x64
        self.up6 = UNetBlock(256, 64, down=False)                        # 128x128
        
        # ìµœì¢… ì¶œë ¥ì¸µ
        self.final = nn.ConvTranspose2d(128, out_channels, 4, 2, 1)      # 256x256
        self.tanh = nn.Tanh()
    
    def forward(self, x):
        # Encoder
        d1 = self.down1(x)
        d2 = self.down2(d1)
        d3 = self.down3(d2)
        d4 = self.down4(d3)
        d5 = self.down5(d4)
        d6 = self.down6(d5)
        
        # Bottleneck
        bottleneck = self.bottleneck(d6)
        
        # Decoder with skip connections
        u1 = self.up1(bottleneck)
        u2 = self.up2(torch.cat([u1, d6], 1))
        u3 = self.up3(torch.cat([u2, d5], 1))
        u4 = self.up4(torch.cat([u3, d4], 1))
        u5 = self.up5(torch.cat([u4, d3], 1))
        u6 = self.up6(torch.cat([u5, d2], 1))
        
        # ìµœì¢… ì¶œë ¥
        output = self.final(torch.cat([u6, d1], 1))
        return self.tanh(output)

class Discriminator(nn.Module):
    """íŒë³„ê¸° (PatchGAN)"""
    def __init__(self, in_channels=6):  # ì…ë ¥ + íƒ€ê²Ÿ ì´ë¯¸ì§€
        super().__init__()
        
        self.model = nn.Sequential(
            nn.Conv2d(in_channels, 64, 4, 2, 1),
            nn.LeakyReLU(0.2, inplace=True),
            
            nn.Conv2d(64, 128, 4, 2, 1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            
            nn.Conv2d(128, 256, 4, 2, 1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            
            nn.Conv2d(256, 512, 4, 1, 1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            
            nn.Conv2d(512, 1, 4, 1, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x, y):
        # ì…ë ¥ê³¼ íƒ€ê²Ÿì„ ì—°ê²°
        combined = torch.cat([x, y], 1)
        return self.model(combined)

# ================================================================
# 4. í›ˆë ¨ ì„¤ì • ë° í•¨ìˆ˜
# ================================================================

def setup_training():
    """í›ˆë ¨ í™˜ê²½ ì„¤ì •"""
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    generator = Generator().to(device)
    discriminator = Discriminator().to(device)
    
    # ì†ì‹¤ í•¨ìˆ˜
    criterion_GAN = nn.BCELoss()
    criterion_L1 = nn.L1Loss()
    
    # ìµœì í™”ê¸°
    optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
    ])
    
    return {
        'device': device,
        'generator': generator,
        'discriminator': discriminator,
        'criterion_GAN': criterion_GAN,
        'criterion_L1': criterion_L1,
        'optimizer_G': optimizer_G,
        'optimizer_D': optimizer_D,
        'transform': transform
    }

def train_one_epoch(dataloader, models_dict, epoch):
    """í•œ ì—í¬í¬ í›ˆë ¨"""
    device = models_dict['device']
    generator = models_dict['generator']
    discriminator = models_dict['discriminator']
    criterion_GAN = models_dict['criterion_GAN']
    criterion_L1 = models_dict['criterion_L1']
    optimizer_G = models_dict['optimizer_G']
    optimizer_D = models_dict['optimizer_D']
    
    generator.train()
    discriminator.train()
    
    running_loss_G = 0.0
    running_loss_D = 0.0
    
    for batch_idx, (source, target) in enumerate(dataloader):
        source, target = source.to(device), target.to(device)
        batch_size = source.size(0)
        
        # ë ˆì´ë¸” ìƒì„±
        real_label = torch.ones(batch_size, 1, 30, 30).to(device)  # PatchGAN ì¶œë ¥ í¬ê¸°ì— ë§ì¶¤
        fake_label = torch.zeros(batch_size, 1, 30, 30).to(device)
        
        # =====================================
        # íŒë³„ê¸° í›ˆë ¨
        # =====================================
        optimizer_D.zero_grad()
        
        # ì‹¤ì œ ì´ë¯¸ì§€ ìŒ íŒë³„
        real_pred = discriminator(source, target)
        loss_D_real = criterion_GAN(real_pred, real_label)
        
        # ê°€ì§œ ì´ë¯¸ì§€ ìƒì„± ë° íŒë³„
        fake_target = generator(source)
        fake_pred = discriminator(source, fake_target.detach())
        loss_D_fake = criterion_GAN(fake_pred, fake_label)
        
        # íŒë³„ê¸° ì´ ì†ì‹¤
        loss_D = (loss_D_real + loss_D_fake) * 0.5
        loss_D.backward()
        optimizer_D.step()
        
        # =====================================
        # ìƒì„±ê¸° í›ˆë ¨
        # =====================================
        optimizer_G.zero_grad()
        
        # íŒë³„ê¸°ë¥¼ ì†ì´ëŠ” ì†ì‹¤
        fake_pred = discriminator(source, fake_target)
        loss_G_GAN = criterion_GAN(fake_pred, real_label)
        
        # L1 ì†ì‹¤ (í”½ì…€ ë‹¨ìœ„ ìœ ì‚¬ì„±)
        loss_G_L1 = criterion_L1(fake_target, target)
        
        # ìƒì„±ê¸° ì´ ì†ì‹¤
        lambda_L1 = 100  # L1 ì†ì‹¤ ê°€ì¤‘ì¹˜
        loss_G = loss_G_GAN + lambda_L1 * loss_G_L1
        loss_G.backward()
        optimizer_G.step()
        
        running_loss_G += loss_G.item()
        running_loss_D += loss_D.item()
        
        # ì§„í–‰ìƒí™© ì¶œë ¥
        if batch_idx % 50 == 0:
            print(f'Epoch [{epoch}], Batch [{batch_idx}/{len(dataloader)}], '
                  f'Loss_G: {loss_G.item():.4f}, Loss_D: {loss_D.item():.4f}')
    
    avg_loss_G = running_loss_G / len(dataloader)
    avg_loss_D = running_loss_D / len(dataloader)
    
    return avg_loss_G, avg_loss_D

# ================================================================
# 5. ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜
# ================================================================

def main_training():
    """ë©”ì¸ í›ˆë ¨ ì‹¤í–‰"""
    
    # ê²½ë¡œ ì„¤ì • (í˜„ì¬ ìƒí™©ì— ë§ê²Œ ìˆ˜ì •)
    current_artwork_dir = r'C:\Users\brigh\Documents\GitHub\warehouse\ë¹„ì§€ë„\artwork_data'
    # source_dir = "path/to/general/photos"  # ì¼ë°˜ ì‚¬ì§„ ë””ë ‰í† ë¦¬ (ì¶”ê°€ í•„ìš”)
    
    print("="*60)
    print("ğŸ¨ Image-to-Image ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹œì‘")
    print("="*60)
    
    # í˜„ì¬ ë°˜ ê³ í ì‘í’ˆë§Œ ìˆëŠ” ìƒí™©
    print(f"í˜„ì¬ ë°˜ ê³ í ì‘í’ˆ ìˆ˜: {len(os.listdir(current_artwork_dir))}")
    print("\nâš ï¸  ì£¼ì˜: ì™„ì „í•œ í›ˆë ¨ì„ ìœ„í•´ì„œëŠ” ì¼ë°˜ ì‚¬ì§„ ë°ì´í„°ê°€ ì¶”ê°€ë¡œ í•„ìš”í•©ë‹ˆë‹¤.")
    print("   Style Transferì˜ ê²½ìš°: ì†ŒìŠ¤ ì´ë¯¸ì§€(ì¼ë°˜ ì‚¬ì§„) + íƒ€ê²Ÿ ì´ë¯¸ì§€(ë°˜ ê³ í ì‘í’ˆ)")
    print("   í˜„ì¬ëŠ” ë°˜ ê³ í ì‘í’ˆë§Œ ìˆì–´ì„œ style transferê°€ ì•„ë‹Œ ë‹¤ë¥¸ ë°©ë²• ê³ ë ¤ í•„ìš”")
    
    # í›ˆë ¨ ì„¤ì •
    config = setup_training()
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„°
    EPOCHS = 100
    BATCH_SIZE = 8
    LEARNING_RATE = 0.0002
    
    print(f"\nğŸ“‹ í›ˆë ¨ ì„¤ì •:")
    print(f"   - ì—í¬í¬ ìˆ˜: {EPOCHS}")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}")
    print(f"   - í•™ìŠµë¥ : {LEARNING_RATE}")
    print(f"   - ë””ë°”ì´ìŠ¤: {config['device']}")
    
    # ì‹¤ì œ ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ë©´ ì•„ë˜ ì½”ë“œ í™œì„±í™”
    """
    # ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±
    dataset = ImageToImageDataset(
        source_dir=source_dir,
        target_dir=current_artwork_dir,
        transform=config['transform'],
        paired=False  # unpaired í•™ìŠµ
    )
    
    dataloader = DataLoader(
        dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True, 
        num_workers=2
    )
    
    # í›ˆë ¨ ì‹¤í–‰
    for epoch in range(EPOCHS):
        avg_loss_G, avg_loss_D = train_one_epoch(dataloader, config, epoch)
        
        print(f'Epoch [{epoch+1}/{EPOCHS}] - Avg Loss_G: {avg_loss_G:.4f}, Avg Loss_D: {avg_loss_D:.4f}')
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if (epoch + 1) % 10 == 0:
            torch.save({
                'epoch': epoch,
                'generator_state_dict': config['generator'].state_dict(),
                'discriminator_state_dict': config['discriminator'].state_dict(),
                'optimizer_G_state_dict': config['optimizer_G'].state_dict(),
                'optimizer_D_state_dict': config['optimizer_D'].state_dict(),
            }, f'checkpoint_epoch_{epoch+1}.pth')
    """

if __name__ == "__main__":
    main_training()
