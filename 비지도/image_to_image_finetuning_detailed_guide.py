# Image-to-Image ëª¨ë¸ íŒŒì¸íŠœë‹ ìƒì„¸ ê°€ì´ë“œ
"""
ì´ë¯¸ì§€ íˆ¬ ì´ë¯¸ì§€ ëª¨ë¸ íŒŒì¸íŠœë‹ ì™„ì „ ê°€ì´ë“œ

ì´ ê°€ì´ë“œëŠ” ë°˜ ê³ í ìŠ¤íƒ€ì¼ ë³€í™˜ì„ ìœ„í•œ Image-to-Image ëª¨ë¸ íŒŒì¸íŠœë‹ ê³¼ì •ì„ 
ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import json
from datetime import datetime

# ================================
# 1. ë°ì´í„°ì…‹ ì¤€ë¹„ ë° êµ¬ì„±
# ================================

class ImageToImageDataset(Dataset):
    """
    Image-to-Image ë³€í™˜ì„ ìœ„í•œ ë°ì´í„°ì…‹ í´ë˜ìŠ¤
    
    êµ¬ì¡°:
    - source_images/: ì›ë³¸ ì´ë¯¸ì§€ë“¤ (ì¼ë°˜ ì‚¬ì§„, í’ê²½ ë“±)
    - target_images/: íƒ€ê²Ÿ ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€ë“¤ (ë°˜ ê³ í ì‘í’ˆë“¤)
    """
    
    def __init__(self, source_dir, target_dir, transform=None, image_size=256):
        self.source_dir = source_dir
        self.target_dir = target_dir
        self.transform = transform
        self.image_size = image_size
        
        # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        self.source_images = self._get_image_files(source_dir)
        self.target_images = self._get_image_files(target_dir)
        
        # ë°ì´í„°ì…‹ í¬ê¸°ëŠ” ë” ì‘ì€ ìª½ì— ë§ì¶¤
        self.dataset_size = min(len(self.source_images), len(self.target_images))
        
        print(f"Source images: {len(self.source_images)}")
        print(f"Target images: {len(self.target_images)}")
        print(f"Dataset size: {self.dataset_size}")
    
    def _get_image_files(self, directory):
        """ë””ë ‰í† ë¦¬ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        extensions = ['.jpg', '.jpeg', '.png', '.bmp']
        image_files = []
        
        for ext in extensions:
            image_files.extend([
                os.path.join(directory, f) 
                for f in os.listdir(directory) 
                if f.lower().endswith(ext)
            ])
        
        return sorted(image_files)
    
    def __len__(self):
        return self.dataset_size
    
    def __getitem__(self, idx):
        # ì¸ë±ìŠ¤ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ìˆœí™˜
        source_idx = idx % len(self.source_images)
        target_idx = idx % len(self.target_images)
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        source_path = self.source_images[source_idx]
        target_path = self.target_images[target_idx]
        
        source_image = Image.open(source_path).convert('RGB')
        target_image = Image.open(target_path).convert('RGB')
        
        # í¬ê¸° ì¡°ì •
        source_image = source_image.resize((self.image_size, self.image_size))
        target_image = target_image.resize((self.image_size, self.image_size))
        
        # ë³€í™˜ ì ìš©
        if self.transform:
            source_image = self.transform(source_image)
            target_image = self.transform(target_image)
        
        return {
            'source': source_image,
            'target': target_image,
            'source_path': source_path,
            'target_path': target_path
        }

# ================================
# 2. ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜
# ================================

class ResidualBlock(nn.Module):
    """ì”ì°¨ ë¸”ë¡ - ìŠ¤íƒ€ì¼ ë³€í™˜ì— íš¨ê³¼ì """
    
    def __init__(self, channels):
        super(ResidualBlock, self).__init__()
        self.conv_block = nn.Sequential(
            nn.Conv2d(channels, channels, 3, padding=1),
            nn.InstanceNorm2d(channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels, channels, 3, padding=1),
            nn.InstanceNorm2d(channels)
        )
    
    def forward(self, x):
        return x + self.conv_block(x)

class Generator(nn.Module):
    """
    Generator ë„¤íŠ¸ì›Œí¬ - U-Net ê¸°ë°˜ êµ¬ì¡°
    
    êµ¬ì¡°:
    1. Encoder: ì´ë¯¸ì§€ë¥¼ ë‚®ì€ í•´ìƒë„ íŠ¹ì„±ë§µìœ¼ë¡œ ë³€í™˜
    2. Residual Blocks: ìŠ¤íƒ€ì¼ ë³€í™˜ ìˆ˜í–‰
    3. Decoder: ì›ë˜ í•´ìƒë„ë¡œ ë³µì›
    """
    
    def __init__(self, input_channels=3, output_channels=3, ngf=64, n_residual_blocks=9):
        super(Generator, self).__init__()
        
        # Encoder
        self.encoder = nn.Sequential(
            # ì²« ë²ˆì§¸ ë ˆì´ì–´
            nn.Conv2d(input_channels, ngf, 7, padding=3),
            nn.InstanceNorm2d(ngf),
            nn.ReLU(inplace=True),
            
            # ë‹¤ìš´ìƒ˜í”Œë§ ë ˆì´ì–´ë“¤
            nn.Conv2d(ngf, ngf * 2, 3, stride=2, padding=1),
            nn.InstanceNorm2d(ngf * 2),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(ngf * 2, ngf * 4, 3, stride=2, padding=1),
            nn.InstanceNorm2d(ngf * 4),
            nn.ReLU(inplace=True),
        )
        
        # Residual blocks
        self.residual_blocks = nn.Sequential(
            *[ResidualBlock(ngf * 4) for _ in range(n_residual_blocks)]
        )
        
        # Decoder
        self.decoder = nn.Sequential(
            # ì—…ìƒ˜í”Œë§ ë ˆì´ì–´ë“¤
            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, stride=2, padding=1, output_padding=1),
            nn.InstanceNorm2d(ngf * 2),
            nn.ReLU(inplace=True),
            
            nn.ConvTranspose2d(ngf * 2, ngf, 3, stride=2, padding=1, output_padding=1),
            nn.InstanceNorm2d(ngf),
            nn.ReLU(inplace=True),
            
            # ìµœì¢… ì¶œë ¥ ë ˆì´ì–´
            nn.Conv2d(ngf, output_channels, 7, padding=3),
            nn.Tanh()  # [-1, 1] ë²”ìœ„ë¡œ ì¶œë ¥
        )
    
    def forward(self, x):
        # Encoder
        encoded = self.encoder(x)
        
        # Residual blocks
        residual = self.residual_blocks(encoded)
        
        # Decoder
        output = self.decoder(residual)
        
        return output

class Discriminator(nn.Module):
    """
    Discriminator ë„¤íŠ¸ì›Œí¬ - PatchGAN êµ¬ì¡°
    
    ì´ë¯¸ì§€ ì „ì²´ê°€ ì•„ë‹Œ íŒ¨ì¹˜ ë‹¨ìœ„ë¡œ ì§„ì§œ/ê°€ì§œë¥¼ íŒë³„í•˜ì—¬
    ë” ì„¸ë°€í•œ í…ìŠ¤ì²˜ì™€ ë””í…Œì¼ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    
    def __init__(self, input_channels=3, ndf=64):
        super(Discriminator, self).__init__()
        
        self.model = nn.Sequential(
            # ì²« ë²ˆì§¸ ë ˆì´ì–´ (ë°°ì¹˜ ì •ê·œí™” ì—†ìŒ)
            nn.Conv2d(input_channels, ndf, 4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            
            # ì¤‘ê°„ ë ˆì´ì–´ë“¤
            nn.Conv2d(ndf, ndf * 2, 4, stride=2, padding=1),
            nn.InstanceNorm2d(ndf * 2),
            nn.LeakyReLU(0.2, inplace=True),
            
            nn.Conv2d(ndf * 2, ndf * 4, 4, stride=2, padding=1),
            nn.InstanceNorm2d(ndf * 4),
            nn.LeakyReLU(0.2, inplace=True),
            
            nn.Conv2d(ndf * 4, ndf * 8, 4, stride=1, padding=1),
            nn.InstanceNorm2d(ndf * 8),
            nn.LeakyReLU(0.2, inplace=True),
            
            # ìµœì¢… ì¶œë ¥ ë ˆì´ì–´
            nn.Conv2d(ndf * 8, 1, 4, stride=1, padding=1)
        )
    
    def forward(self, x):
        return self.model(x)

# ================================
# 3. ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
# ================================

class PerceptualLoss(nn.Module):
    """
    Perceptual Loss - VGG ë„¤íŠ¸ì›Œí¬ì˜ íŠ¹ì„±ë§µì„ ì´ìš©í•œ ì†ì‹¤
    
    í”½ì…€ ë‹¨ìœ„ ì°¨ì´ë³´ë‹¤ëŠ” ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ì¸¡ì •í•˜ì—¬
    ë” ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤íƒ€ì¼ ë³€í™˜ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        super(PerceptualLoss, self).__init__()
        # ì‚¬ì „ í›ˆë ¨ëœ VGG19 ëª¨ë¸ ì‚¬ìš©
        vgg = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True)
        
        # VGGì˜ íŠ¹ì • ë ˆì´ì–´ë“¤ë§Œ ì‚¬ìš©
        self.features = nn.Sequential(*list(vgg.features)[:36]).eval()
        
        # íŒŒë¼ë¯¸í„° ê³ ì •
        for param in self.features.parameters():
            param.requires_grad = False
    
    def forward(self, generated, target):
        # VGG íŠ¹ì„±ë§µ ì¶”ì¶œ
        generated_features = self.features(generated)
        target_features = self.features(target)
        
        # MSE ì†ì‹¤ ê³„ì‚°
        loss = nn.MSELoss()(generated_features, target_features)
        return loss

def gradient_penalty(discriminator, real_samples, fake_samples, device):
    """
    WGAN-GPë¥¼ ìœ„í•œ Gradient Penalty ê³„ì‚°
    
    ë” ì•ˆì •ì ì¸ GAN í•™ìŠµì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.
    """
    batch_size = real_samples.size(0)
    
    # ëœë¤ ë³´ê°„ ê³„ìˆ˜
    alpha = torch.rand(batch_size, 1, 1, 1).to(device)
    
    # ì‹¤ì œì™€ ê°€ì§œ ìƒ˜í”Œ ì‚¬ì´ì˜ ë³´ê°„
    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)
    
    # Discriminatorë¥¼ í†µê³¼
    d_interpolates = discriminator(interpolates)
    
    # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
    gradients = torch.autograd.grad(
        outputs=d_interpolates,
        inputs=interpolates,
        grad_outputs=torch.ones_like(d_interpolates),
        create_graph=True,
        retain_graph=True,
        only_inputs=True
    )[0]
    
    # Gradient penalty ê³„ì‚°
    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
    return gradient_penalty

# ================================
# 4. í•™ìŠµ ì„¤ì • ë° ì‹¤í–‰
# ================================

class StyleTransferTrainer:
    """Image-to-Image ìŠ¤íƒ€ì¼ ë³€í™˜ ëª¨ë¸ í›ˆë ¨ í´ë˜ìŠ¤"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.generator = Generator().to(self.device)
        self.discriminator = Discriminator().to(self.device)
        
        # ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
        self.g_optimizer = optim.Adam(
            self.generator.parameters(),
            lr=config['g_lr'],
            betas=(0.5, 0.999)
        )
        
        self.d_optimizer = optim.Adam(
            self.discriminator.parameters(),
            lr=config['d_lr'],
            betas=(0.5, 0.999)
        )
        
        # ì†ì‹¤ í•¨ìˆ˜ ì´ˆê¸°í™”
        self.adversarial_loss = nn.MSELoss()
        self.l1_loss = nn.L1Loss()
        self.perceptual_loss = PerceptualLoss().to(self.device)
        
        # í•™ìŠµ ê¸°ë¡
        self.losses = {
            'g_total': [],
            'g_adversarial': [],
            'g_l1': [],
            'g_perceptual': [],
            'd_real': [],
            'd_fake': []
        }
    
    def train_epoch(self, dataloader, epoch):
        """í•œ ì—í­ í›ˆë ¨"""
        self.generator.train()
        self.discriminator.train()
        
        epoch_losses = {key: 0.0 for key in self.losses.keys()}
        
        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}')
        
        for batch_idx, batch in enumerate(progress_bar):
            source_images = batch['source'].to(self.device)
            target_images = batch['target'].to(self.device)
            batch_size = source_images.size(0)
            
            # ============================
            # Discriminator í›ˆë ¨
            # ============================
            
            self.d_optimizer.zero_grad()
            
            # ì§„ì§œ ì´ë¯¸ì§€ì— ëŒ€í•œ íŒë³„
            real_pred = self.discriminator(target_images)
            real_labels = torch.ones_like(real_pred)
            d_real_loss = self.adversarial_loss(real_pred, real_labels)
            
            # ê°€ì§œ ì´ë¯¸ì§€ ìƒì„± ë° íŒë³„
            with torch.no_grad():
                fake_images = self.generator(source_images)
            fake_pred = self.discriminator(fake_images.detach())
            fake_labels = torch.zeros_like(fake_pred)
            d_fake_loss = self.adversarial_loss(fake_pred, fake_labels)
            
            # Discriminator ì´ ì†ì‹¤
            d_loss = (d_real_loss + d_fake_loss) / 2
            d_loss.backward()
            self.d_optimizer.step()
            
            # ============================
            # Generator í›ˆë ¨
            # ============================
            
            self.g_optimizer.zero_grad()
            
            # ê°€ì§œ ì´ë¯¸ì§€ ìƒì„±
            fake_images = self.generator(source_images)
            
            # Adversarial Loss
            fake_pred = self.discriminator(fake_images)
            real_labels = torch.ones_like(fake_pred)
            g_adversarial_loss = self.adversarial_loss(fake_pred, real_labels)
            
            # L1 Loss (í”½ì…€ ë‹¨ìœ„ ì°¨ì´)
            g_l1_loss = self.l1_loss(fake_images, target_images)
            
            # Perceptual Loss (ì˜ë¯¸ì  ìœ ì‚¬ì„±)
            g_perceptual_loss = self.perceptual_loss(fake_images, target_images)
            
            # Generator ì´ ì†ì‹¤
            g_total_loss = (
                self.config['lambda_adv'] * g_adversarial_loss +
                self.config['lambda_l1'] * g_l1_loss +
                self.config['lambda_perceptual'] * g_perceptual_loss
            )
            
            g_total_loss.backward()
            self.g_optimizer.step()
            
            # ì†ì‹¤ ê¸°ë¡
            epoch_losses['g_total'] += g_total_loss.item()
            epoch_losses['g_adversarial'] += g_adversarial_loss.item()
            epoch_losses['g_l1'] += g_l1_loss.item()
            epoch_losses['g_perceptual'] += g_perceptual_loss.item()
            epoch_losses['d_real'] += d_real_loss.item()
            epoch_losses['d_fake'] += d_fake_loss.item()
            
            # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
            progress_bar.set_postfix({
                'G_loss': f'{g_total_loss.item():.4f}',
                'D_loss': f'{d_loss.item():.4f}'
            })
        
        # ì—í­ í‰ê·  ì†ì‹¤ ê³„ì‚°
        num_batches = len(dataloader)
        for key in epoch_losses:
            epoch_losses[key] /= num_batches
            self.losses[key].append(epoch_losses[key])
        
        return epoch_losses
    
    def save_checkpoint(self, epoch, save_dir):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'epoch': epoch,
            'generator_state_dict': self.generator.state_dict(),
            'discriminator_state_dict': self.discriminator.state_dict(),
            'g_optimizer_state_dict': self.g_optimizer.state_dict(),
            'd_optimizer_state_dict': self.d_optimizer.state_dict(),
            'losses': self.losses,
            'config': self.config
        }
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}_{timestamp}.pth')
        torch.save(checkpoint, checkpoint_path)
        
        print(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ë¨: {checkpoint_path}")
        return checkpoint_path
    
    def generate_samples(self, dataloader, save_dir, num_samples=5):
        """ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„± ë° ì €ì¥"""
        self.generator.eval()
        
        with torch.no_grad():
            batch = next(iter(dataloader))
            source_images = batch['source'][:num_samples].to(self.device)
            target_images = batch['target'][:num_samples].to(self.device)
            
            # ì´ë¯¸ì§€ ìƒì„±
            generated_images = self.generator(source_images)
            
            # ì‹œê°í™”
            fig, axes = plt.subplots(3, num_samples, figsize=(15, 9))
            
            for i in range(num_samples):
                # ì •ê·œí™” í•´ì œ [-1, 1] -> [0, 1]
                source = (source_images[i].cpu() + 1) / 2
                target = (target_images[i].cpu() + 1) / 2
                generated = (generated_images[i].cpu() + 1) / 2
                
                # ì´ë¯¸ì§€ í‘œì‹œ
                axes[0, i].imshow(source.permute(1, 2, 0))
                axes[0, i].set_title('Source')
                axes[0, i].axis('off')
                
                axes[1, i].imshow(generated.permute(1, 2, 0))
                axes[1, i].set_title('Generated')
                axes[1, i].axis('off')
                
                axes[2, i].imshow(target.permute(1, 2, 0))
                axes[2, i].set_title('Target')
                axes[2, i].axis('off')
            
            plt.tight_layout()
            
            # ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            sample_path = os.path.join(save_dir, f'samples_{timestamp}.png')
            plt.savefig(sample_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            print(f"ìƒ˜í”Œ ì´ë¯¸ì§€ ì €ì¥ë¨: {sample_path}")

# ================================
# 5. ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ
# ================================

def setup_training_config():
    """í›ˆë ¨ ì„¤ì • êµ¬ì„±"""
    config = {
        # ë°ì´í„°
        'source_dir': r'C:\Users\brigh\Documents\GitHub\warehouse\ë¹„ì§€ë„\source_images',
        'target_dir': r'C:\Users\brigh\Documents\GitHub\warehouse\ë¹„ì§€ë„\artwork_data',
        'image_size': 256,
        'batch_size': 4,
        
        # í›ˆë ¨
        'num_epochs': 100,
        'g_lr': 0.0002,
        'd_lr': 0.0002,
        
        # ì†ì‹¤ í•¨ìˆ˜ ê°€ì¤‘ì¹˜
        'lambda_adv': 1.0,      # Adversarial loss ê°€ì¤‘ì¹˜
        'lambda_l1': 100.0,     # L1 loss ê°€ì¤‘ì¹˜
        'lambda_perceptual': 10.0,  # Perceptual loss ê°€ì¤‘ì¹˜
        
        # ì €ì¥
        'save_dir': r'C:\Users\brigh\Documents\GitHub\warehouse\ë¹„ì§€ë„\training_results',
        'save_interval': 10,    # ëª‡ ì—í­ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        'sample_interval': 5    # ëª‡ ì—í­ë§ˆë‹¤ ìƒ˜í”Œ ìƒì„±
    }
    
    return config

def create_directories(config):
    """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
    directories = [
        config['source_dir'],
        config['target_dir'],
        config['save_dir'],
        os.path.join(config['save_dir'], 'checkpoints'),
        os.path.join(config['save_dir'], 'samples')
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"ë””ë ‰í† ë¦¬ í™•ì¸/ìƒì„±: {directory}")

def setup_data_transforms(image_size):
    """ë°ì´í„° ì „ì²˜ë¦¬ ë³€í™˜ ì„¤ì •"""
    transform = transforms.Compose([
        transforms.Resize((image_size, image_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # [-1, 1]ë¡œ ì •ê·œí™”
    ])
    
    return transform

def main_training():
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    print("ğŸ¨ Image-to-Image ìŠ¤íƒ€ì¼ ë³€í™˜ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
    print("="*70)
    
    # ì„¤ì • ë¡œë“œ
    config = setup_training_config()
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    create_directories(config)
    
    # ë””ë°”ì´ìŠ¤ í™•ì¸
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # ë°ì´í„° ì „ì²˜ë¦¬ ì„¤ì •
    transform = setup_data_transforms(config['image_size'])
    
    # ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±
    try:
        dataset = ImageToImageDataset(
            source_dir=config['source_dir'],
            target_dir=config['target_dir'],
            transform=transform,
            image_size=config['image_size']
        )
        
        dataloader = DataLoader(
            dataset,
            batch_size=config['batch_size'],
            shuffle=True,
            num_workers=2 if device.type == 'cuda' else 0
        )
        
        print(f"ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}")
        print(f"ë°°ì¹˜ í¬ê¸°: {config['batch_size']}")
        print(f"ë°°ì¹˜ ìˆ˜: {len(dataloader)}")
        
    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("\ní•´ê²° ë°©ë²•:")
        print("1. source_images í´ë”ì— ì¼ë°˜ ì‚¬ì§„ë“¤ì„ ì¶”ê°€í•˜ì„¸ìš”")
        print("2. artwork_data í´ë”ì— ë°˜ ê³ í ì‘í’ˆë“¤ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
        return
    
    # í›ˆë ¨ ê°ì²´ ìƒì„±
    trainer = StyleTransferTrainer(config)
    
    # í›ˆë ¨ ì‹¤í–‰
    print(f"\nğŸš€ í›ˆë ¨ ì‹œì‘ - {config['num_epochs']} ì—í­")
    print("="*70)
    
    for epoch in range(config['num_epochs']):
        # ì—í­ í›ˆë ¨
        epoch_losses = trainer.train_epoch(dataloader, epoch)
        
        # ì†ì‹¤ ì¶œë ¥
        print(f"\nEpoch {epoch+1}/{config['num_epochs']} ì™„ë£Œ")
        print(f"Generator Loss: {epoch_losses['g_total']:.4f}")
        print(f"Discriminator Loss: {(epoch_losses['d_real'] + epoch_losses['d_fake'])/2:.4f}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if (epoch + 1) % config['save_interval'] == 0:
            checkpoint_dir = os.path.join(config['save_dir'], 'checkpoints')
            trainer.save_checkpoint(epoch, checkpoint_dir)
        
        # ìƒ˜í”Œ ìƒì„±
        if (epoch + 1) % config['sample_interval'] == 0:
            sample_dir = os.path.join(config['save_dir'], 'samples')
            trainer.generate_samples(dataloader, sample_dir)
    
    print("\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_checkpoint_dir = os.path.join(config['save_dir'], 'checkpoints')
    final_checkpoint = trainer.save_checkpoint(config['num_epochs']-1, final_checkpoint_dir)
    
    print(f"ìµœì¢… ëª¨ë¸ ì €ì¥ë¨: {final_checkpoint}")

# ================================
# 6. ëª¨ë¸ ì‚¬ìš© ë° ì¶”ë¡ 
# ================================

def load_trained_model(checkpoint_path, device):
    """í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ"""
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # ëª¨ë¸ ìƒì„±
    generator = Generator().to(device)
    generator.load_state_dict(checkpoint['generator_state_dict'])
    generator.eval()
    
    print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {checkpoint_path}")
    print(f"í›ˆë ¨ ì—í­: {checkpoint['epoch'] + 1}")
    
    return generator

def style_transfer_inference(generator, input_image_path, output_path, device):
    """ìŠ¤íƒ€ì¼ ë³€í™˜ ì¶”ë¡ """
    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    
    # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
    input_image = Image.open(input_image_path).convert('RGB')
    input_tensor = transform(input_image).unsqueeze(0).to(device)
    
    # ìŠ¤íƒ€ì¼ ë³€í™˜
    with torch.no_grad():
        output_tensor = generator(input_tensor)
        
        # ì •ê·œí™” í•´ì œ
        output_tensor = (output_tensor + 1) / 2
        output_tensor = torch.clamp(output_tensor, 0, 1)
        
        # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        output_image = transforms.ToPILImage()(output_tensor.squeeze(0).cpu())
        
        # ì €ì¥
        output_image.save(output_path)
        
        print(f"ìŠ¤íƒ€ì¼ ë³€í™˜ ì™„ë£Œ: {output_path}")
        
        return output_image

if __name__ == "__main__":
    print("Image-to-Image ëª¨ë¸ íŒŒì¸íŠœë‹ ê°€ì´ë“œ")
    print("="*50)
    print()
    print("ì´ íŒŒì¼ì€ ë°˜ ê³ í ìŠ¤íƒ€ì¼ ë³€í™˜ì„ ìœ„í•œ")
    print("Image-to-Image ëª¨ë¸ íŒŒì¸íŠœë‹ ê°€ì´ë“œì…ë‹ˆë‹¤.")
    print()
    print("ì‹¤í–‰í•˜ë ¤ë©´:")
    print("1. source_images í´ë”ì— ì¼ë°˜ ì‚¬ì§„ë“¤ ì¶”ê°€")
    print("2. artwork_data í´ë”ì— ë°˜ ê³ í ì‘í’ˆë“¤ í™•ì¸")
    print("3. main_training() í•¨ìˆ˜ ì‹¤í–‰")
    print()
    print("ì£¼ì˜: GPUê°€ ê¶Œì¥ë˜ë©°, ì¶©ë¶„í•œ ì €ì¥ê³µê°„ì´ í•„ìš”í•©ë‹ˆë‹¤.")
